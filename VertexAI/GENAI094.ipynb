{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e321cff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfaecaec",
   "metadata": {},
   "source": [
    "### Deploy Stable Diffusion XL via the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189ed8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Shell coomads:\n",
    "\n",
    "$ gcloud auth application-default login\n",
    "\n",
    "$ gcloud config set billing/quota_project qwiklabs-asl-04-1e4c51b2847c\n",
    "\n",
    "$ gcloud ai model-garden models deploy \\\n",
    "--model=\"stability-ai/stable-diffusion-xl-base@stable-diffusion-xl-base-1.0\" \\\n",
    "--region=\"us-central1\" \\\n",
    "--project=\"qwiklabs-asl-04-1e4c51b2847c\" \\\n",
    "--accept-eula \\\n",
    "--machine-type=\"g2-standard-8\" \\\n",
    "--accelerator-type=\"NVIDIA_L4\" \\\n",
    "--container-image-uri=\"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/pytorch-inference.cu125.0-1.ubuntu2204.py310\" \\\n",
    "--endpoint-display-name=\"stabilityai_stable-diffusion-xl-1-mg-one-click-deploy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9c956c",
   "metadata": {},
   "source": [
    "### Deploy the Falcon Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11bc0b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGiven that Falcon-instruct is being fine-tuned using PEFT, which of the following is a common technique used in PEFT to reduce the number of trainable parameters?\\n  -  Training all the layers of the original Falcon model from scratch\\n  -  Converting the model architecture to a recurrent neural network (RNN)\\n (+) Adding small, trainable adapter layers to the existing Falcon model while freezing most of its original parameters\\n  -  Using a significantly smaller training dataset compared to the original Falcon model\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review the card information and answer the following question:\n",
    "\n",
    "'''\n",
    "Given that Falcon-instruct is being fine-tuned using PEFT, which of the following is a common technique used in PEFT to reduce the number of trainable parameters?\n",
    "  -  Training all the layers of the original Falcon model from scratch\n",
    "  -  Converting the model architecture to a recurrent neural network (RNN)\n",
    " (+) Adding small, trainable adapter layers to the existing Falcon model while freezing most of its original parameters\n",
    "  -  Using a significantly smaller training dataset compared to the original Falcon model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6410bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai import model_garden\n",
    "\n",
    "vertexai.init(project=\"qwiklabs-asl-04-1e4c51b2847c\", location=\"us-central1\")\n",
    "\n",
    "model = model_garden.OpenModel(\"tiiuae/falcon-instruct-7b-peft@falcon-7b-instruct\")\n",
    "endpoint = model.deploy(\n",
    "  accept_eula=True,\n",
    "  machine_type=\"g2-standard-12\",\n",
    "  accelerator_type=\"NVIDIA_L4\",\n",
    "  accelerator_count=1,\n",
    "  serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240620_1616_RC00\",\n",
    "  endpoint_display_name=\"falcon-instruct-7b-peft-deploy-challenge-lab\",\n",
    "  model_display_name=\"falcon-instruct-7b-peft-001-1750287659003\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
