{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75a2d348",
   "metadata": {},
   "source": [
    "# Getting Started with Google Search as a Tool with Gemini in Vertex AI\n",
    "\n",
    "https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/grounding/intro-grounding-gemini.ipynb\n",
    "\n",
    "### Set up the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997effd0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d6acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install Google Gen AI SDK for Python\n",
    "# Install the following packages required to execute this notebook.\n",
    "\n",
    "%pip install --upgrade --quiet google-genai\n",
    "\n",
    "# Restart runtime\n",
    "# To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel.\n",
    "\n",
    "import IPython\n",
    "​\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n",
    "\n",
    "# -->⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️\n",
    "\n",
    "\n",
    "# 2. Authenticate your Google Cloud account\n",
    "# If you are running this notebook on Google Colab, you will need to authenticate your environment. To do this, run the new cell below. This step is not required if you are using Vertex AI Workbench.\n",
    "\n",
    "import sys\n",
    "​\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Authenticate user to Google Cloud\n",
    "    from google.colab import auth\n",
    "​\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1bcff1",
   "metadata": {},
   "source": [
    "### Set Google Cloud project information and create client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1f94bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project information\n",
    "PROJECT_ID = \"qwiklabs-gcp-00-3e05f4fe2c5f\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-west1\"  # @param {type:\"string\"}\n",
    "\n",
    "# Create the API client\n",
    "from google import genai\n",
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05da25eb",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb79add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from google.genai.types import (\n",
    "    ApiKeyConfig,\n",
    "    AuthConfig,\n",
    "    EnterpriseWebSearch,\n",
    "    GenerateContentConfig,\n",
    "    GenerateContentResponse,\n",
    "    GoogleMaps,\n",
    "    GoogleSearch,\n",
    "    LatLng,\n",
    "    Part,\n",
    "    Retrieval,\n",
    "    RetrievalConfig,\n",
    "    Tool,\n",
    "    ToolConfig,\n",
    "    VertexAISearch,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e140888",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ba94a7",
   "metadata": {},
   "source": [
    "def print_grounding_data(response: GenerateContentResponse) -> None:\n",
    "    \"\"\"Prints Gemini response with grounding citations in Markdown format.\"\"\"\n",
    "    if not (response.candidates and response.candidates[0].grounding_metadata):\n",
    "        print(\"Response does not contain grounding metadata.\")\n",
    "        display(Markdown(response.text))\n",
    "        return\n",
    "\n",
    "    grounding_metadata = response.candidates[0].grounding_metadata\n",
    "    markdown_parts = []\n",
    "\n",
    "    # Citation indexes are in bytes\n",
    "    ENCODING = \"utf-8\"\n",
    "    text_bytes = response.text.encode(ENCODING)\n",
    "    last_byte_index = 0\n",
    "\n",
    "    for support in grounding_metadata.grounding_supports:\n",
    "        markdown_parts.append(\n",
    "            text_bytes[last_byte_index : support.segment.end_index].decode(ENCODING)\n",
    "        )\n",
    "\n",
    "        # Generate and append citation footnotes (e.g., \"[1][2]\")\n",
    "        footnotes = \"\".join([f\"[{i + 1}]\" for i in support.grounding_chunk_indices])\n",
    "        markdown_parts.append(f\" {footnotes}\")\n",
    "\n",
    "        # Update index for the next segment\n",
    "        last_byte_index = support.segment.end_index\n",
    "\n",
    "    # Append any remaining text after the last citation\n",
    "    if last_byte_index < len(text_bytes):\n",
    "        markdown_parts.append(text_bytes[last_byte_index:].decode(ENCODING))\n",
    "\n",
    "    markdown_parts.append(\"\\n\\n----\\n## Grounding Sources\\n\")\n",
    "\n",
    "    # Build Grounding Sources Section\n",
    "    markdown_parts.append(\"### Grounding Chunks\\n\")\n",
    "    for i, chunk in enumerate(grounding_metadata.grounding_chunks, start=1):\n",
    "        context = chunk.web or chunk.retrieved_context\n",
    "        if not context:\n",
    "            continue\n",
    "\n",
    "        uri = context.uri\n",
    "        title = context.title or \"Source\"\n",
    "\n",
    "        # Convert GCS URIs to public HTTPS URLs\n",
    "        if uri and uri.startswith(\"gs://\"):\n",
    "            uri = uri.replace(\"gs://\", \"https://storage.googleapis.com/\", 1).replace(\n",
    "                \" \", \"%20\"\n",
    "            )\n",
    "        markdown_parts.append(f\"{i}. [{title}]({uri})\\n\")\n",
    "\n",
    "    # Add Search/Retrieval Queries\n",
    "    if grounding_metadata.web_search_queries:\n",
    "        markdown_parts.append(\n",
    "            f\"\\n**Web Search Queries:** {grounding_metadata.web_search_queries}\\n\"\n",
    "        )\n",
    "        if grounding_metadata.search_entry_point:\n",
    "            markdown_parts.append(\n",
    "                f\"\\n**Search Entry Point:**\\n{grounding_metadata.search_entry_point.rendered_content}\\n\"\n",
    "            )\n",
    "    elif grounding_metadata.retrieval_queries:\n",
    "        markdown_parts.append(\n",
    "            f\"\\n**Retrieval Queries:** {grounding_metadata.retrieval_queries}\\n\"\n",
    "        )\n",
    "\n",
    "    display(Markdown(\"\".join(markdown_parts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d570f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.0-flash\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d017ac5",
   "metadata": {},
   "source": [
    "### Example: Text generation without grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1c9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"When is the next solar eclipse in the US?\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=PROMPT,\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd9ab8",
   "metadata": {},
   "source": [
    "### Example: Text generation grounded in Google Search results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576c3f5b",
   "metadata": {},
   "source": [
    "google_search_tool = Tool(google_search=GoogleSearch())\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=PROMPT,\n",
    "    config=GenerateContentConfig(tools=[google_search_tool]),\n",
    ")\n",
    "\n",
    "print_grounding_data(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d6ed2",
   "metadata": {},
   "source": [
    "### Example: Text generation with multimodal input grounded in Google Search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39506e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"What is the current temperature at this location?\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"gs://github-repo/generative-ai/gemini/grounding/paris.jpg\",\n",
    "            mime_type=\"image/jpeg\",\n",
    "        ),\n",
    "        PROMPT,\n",
    "    ],\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[google_search_tool],\n",
    "    ),\n",
    ")\n",
    "\n",
    "print_grounding_data(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4754c7fa",
   "metadata": {},
   "source": [
    "### Example: Grounding with custom documents and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a317a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERTEX_AI_SEARCH_PROJECT_ID = PROJECT_ID  # @param {type: \"string\"}\n",
    "VERTEX_AI_SEARCH_REGION = \"global\"  # @param {type: \"string\"}\n",
    "# Replace this with your App (Engine) ID from Vertex AI Search\n",
    "VERTEX_AI_SEARCH_APP_ID = \"my-vertex-ai-search-app_1749704883397\"  # @param {type: \"string\"}\n",
    "\n",
    "VERTEX_AI_SEARCH_ENGINE_NAME = f\"projects/{VERTEX_AI_SEARCH_PROJECT_ID}/locations/{VERTEX_AI_SEARCH_REGION}/collections/default_collection/engines/{VERTEX_AI_SEARCH_APP_ID}\"\n",
    "\n",
    "PROMPT = \"What is the Google company culture like?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation without grounding\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=PROMPT,\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d3b37",
   "metadata": {},
   "source": [
    "# Text generation grounded in Vertex AI Search results\n",
    "vertex_ai_search_tool = Tool(\n",
    "    retrieval=Retrieval(\n",
    "        vertex_ai_search=VertexAISearch(engine=VERTEX_AI_SEARCH_ENGINE_NAME)\n",
    "    )\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What is the company culture like?\",\n",
    "    config=GenerateContentConfig(tools=[vertex_ai_search_tool]),\n",
    ")\n",
    "\n",
    "# print_grounding_data(response)\n",
    "# print(response)\n",
    "display(Markdown(response.text))\n",
    "\n",
    "# Note that the response without grounding doesn't have any context about what company we are asking about. Whereas the response that was grounded in Vertex AI Search results contains information from the documents provided, along with citations of the information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079d6972",
   "metadata": {},
   "source": [
    "### Example: Grounded chat responses\n",
    "\n",
    "You can also use grounding when using chat conversations in Vertex AI. In this example, you'll compare LLM responses with no grounding with responses that are grounded in the results of a Google Search and a data store in Vertex AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ec0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"What are managed datasets in Vertex AI?\"\n",
    "PROMPT_FOLLOWUP = \"What types of data can I use?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f77c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can add the tools keyword arg with a Tool of GoogleSearch to instruct the chat model to first perform a Google Search with the prompt, then construct an answer based on the web search results:\n",
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(tools=[Tool(google_search=GoogleSearch())]),\n",
    ")\n",
    "\n",
    "display(Markdown(\"## Prompt\"))\n",
    "display(Markdown(f\"> {PROMPT}\"))\n",
    "response = chat.send_message(PROMPT)\n",
    "print_grounding_data(response)\n",
    "\n",
    "display(Markdown(\"---\\n\"))\n",
    "\n",
    "display(Markdown(\"## Follow-up Prompt\"))\n",
    "display(Markdown(f\"> {PROMPT_FOLLOWUP}\"))\n",
    "response = chat.send_message(PROMPT_FOLLOWUP)\n",
    "print_grounding_data(response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
