{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.evaluation import EvalTask\n",
    "# Import agent evaluation metrics from the preview module\n",
    "from vertexai.preview.evaluation import metrics\n",
    "from vertexai.generative_models import GenerativeModel # Only if you're evaluating a pure text model too, otherwise optional\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Replace with your actual GCP Project ID and Region\n",
    "PROJECT_ID = \"your-gcp-project-id\"\n",
    "LOCATION = \"us-central1\" # e.g., \"us-central1\"\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "print(f\"Initializing Vertex AI with project: {PROJECT_ID}, location: {LOCATION}\")\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# --- Define Your Agent (as a Callable) ---\n",
    "# This is a placeholder. In a real scenario, this 'callable_agent'\n",
    "# would be a function or method that calls your deployed Vertex AI Agent\n",
    "# and captures its tool calls and final response.\n",
    "# The returned dictionary MUST contain \"predicted_trajectory\" and\n",
    "# optionally \"response\" (if you're evaluating the text response).\n",
    "# The format for predicted_trajectory should be a list of tool call dictionaries,\n",
    "# e.g., [{\"tool_name\": \"my_tool\", \"parameters\": {\"param1\": \"value\"}}]\n",
    "\n",
    "def my_vertex_ai_agent_simulator(user_input: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Simulates a Vertex AI Agent's behavior for demonstration.\n",
    "    In a real application, this would interact with your deployed agent.\n",
    "    \"\"\"\n",
    "    predicted_trajectory = []\n",
    "    final_response = \"\"\n",
    "\n",
    "    # Simple logic to simulate tool selection based on keywords\n",
    "    if \"weather\" in user_input.lower():\n",
    "        predicted_trajectory.append({\"tool_name\": \"get_weather\", \"parameters\": {\"location\": \"Wildomar\"}})\n",
    "        final_response = f\"I am checking the weather for Wildomar based on your request: '{user_input}'.\"\n",
    "    elif \"news\" in user_input.lower():\n",
    "        predicted_trajectory.append({\"tool_name\": \"get_news\", \"parameters\": {\"topic\": \"AI\"}})\n",
    "        final_response = f\"Fetching the latest news on AI based on your request: '{user_input}'.\"\n",
    "    elif \"reminder\" in user_input.lower():\n",
    "        predicted_trajectory.append({\"tool_name\": \"set_reminder\", \"parameters\": {\"time\": \"tomorrow 9 AM\", \"task\": \"call friend\"}})\n",
    "        final_response = f\"I am setting a reminder for you based on: '{user_input}'.\"\n",
    "    else:\n",
    "        final_response = f\"I couldn't find a specific tool for your request: '{user_input}'. Providing a general response.\"\n",
    "\n",
    "    return {\n",
    "        \"predicted_trajectory\": predicted_trajectory,\n",
    "        \"response\": final_response\n",
    "    }\n",
    "\n",
    "# Assign your agent callable to a variable that EvalTask can use\n",
    "callable_agent = my_vertex_ai_agent_simulator\n",
    "\n",
    "# --- Prepare Your Evaluation Dataset ---\n",
    "# This DataFrame should contain:\n",
    "#   - 'input': The user query to send to the agent.\n",
    "#   - 'reference_trajectory': The ground truth (expected) tool calls for the agent.\n",
    "#     This should be a list of dictionaries, matching the format your agent's\n",
    "#     predicted_trajectory returns. Use an empty list if no tool is expected.\n",
    "#   - 'reference_response' (Optional): The expected final text response from the agent.\n",
    "#     Used if you're also evaluating the text generation quality.\n",
    "\n",
    "eval_dataset = pd.DataFrame({\n",
    "    \"input\": [\n",
    "        \"What's the weather like in Wildomar right now?\",\n",
    "        \"Tell me the latest news about artificial intelligence.\",\n",
    "        \"Can you help me set a reminder for tomorrow?\",\n",
    "        \"What is the capital of California?\", # Expected: No tool used\n",
    "        \"Check the weather in London.\" # Another weather query\n",
    "    ],\n",
    "    \"reference_trajectory\": [\n",
    "        [{\"tool_name\": \"get_weather\", \"parameters\": {\"location\": \"Wildomar\"}}],\n",
    "        [{\"tool_name\": \"get_news\", \"parameters\": {\"topic\": \"AI\"}}],\n",
    "        [{\"tool_name\": \"set_reminder\", \"parameters\": {\"time\": \"tomorrow\", \"task\": \"call friend\"}}],\n",
    "        [], # No tool expected for this\n",
    "        [{\"tool_name\": \"get_weather\", \"parameters\": {\"location\": \"London\"}}]\n",
    "    ],\n",
    "    \"reference_response\": [\n",
    "        \"The current weather in Wildomar is sunny.\",\n",
    "        \"Here's the latest on AI: [summary of news].\",\n",
    "        \"Reminder successfully set for tomorrow.\",\n",
    "        \"The capital of California is Sacramento.\",\n",
    "        \"The weather in London is cloudy.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n--- Agent Evaluation Dataset (first 3 rows) ---\")\n",
    "print(eval_dataset.head(3))\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- Define Agent Evaluation Metrics ---\n",
    "# Use metrics from vertexai.preview.evaluation.metrics for agent evaluation.\n",
    "# TrajectorySingleToolUse checks if a specific tool was present in the trajectory.\n",
    "# You can define multiple such metrics for different tools.\n",
    "\n",
    "agent_metrics = [\n",
    "    metrics.TrajectorySingleToolUse(tool_name=\"get_weather\"), # Check if 'get_weather' tool was used\n",
    "    metrics.TrajectorySingleToolUse(tool_name=\"get_news\"),    # Check if 'get_news' tool was used\n",
    "    metrics.TrajectorySingleToolUse(tool_name=\"set_reminder\"),# Check if 'set_reminder' tool was used\n",
    "    # Additional common agent metrics you might find useful:\n",
    "    metrics.TrajectoryExactMatch(),    # Did the predicted trajectory exactly match the reference?\n",
    "    metrics.TrajectoryAnyOrderMatch(), # Were all reference tools used, regardless of order?\n",
    "    metrics.TrajectoryPrecision(),     # What proportion of predicted tools were correct?\n",
    "    metrics.TrajectoryRecall(),        # What proportion of correct tools were predicted?\n",
    "    # You can also add metrics for the agent's final text response if desired,\n",
    "    # e.g., from MetricPromptTemplateExamples or PointwiseMetric\n",
    "    # PointwiseMetric(\n",
    "    #     metric=\"response_quality\",\n",
    "    #     metric_prompt_template=MetricPromptTemplateExamples.get_prompt_template(\"overall_quality\")\n",
    "    # )\n",
    "]\n",
    "\n",
    "# --- Create and Run the EvalTask ---\n",
    "# For agent evaluation, you use 'runnable' instead of 'model'.\n",
    "# And specify agent-specific column names.\n",
    "\n",
    "print(f\"\\n--- Starting Agent Evaluation with {len(eval_dataset)} instances ---\")\n",
    "\n",
    "eval_task = EvalTask(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=agent_metrics,\n",
    "    # Specify the column names in your eval_dataset and in the runnable's output.\n",
    "    agent_prediction_column_name=\"predicted_trajectory\", # Key in the dict returned by callable_agent\n",
    "    reference_column_name=\"reference_trajectory\",     # Column in eval_dataset for ground truth tools\n",
    "    response_column_name=\"response\", # Key in the dict returned by callable_agent for its text response\n",
    "    input_column_name=\"input\" # The column from your dataset that serves as input to the agent\n",
    ")\n",
    "\n",
    "result = eval_task.evaluate(\n",
    "    runnable=callable_agent, # Pass your agent (or its wrapper function) here\n",
    "    experiment_run_name=\"agent-single-tool-selection-full-example\"\n",
    ")\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\n--- Agent Evaluation Metrics (Aggregate Scores) ---\")\n",
    "# This will show a summary score for each metric (e.g., average precision/recall)\n",
    "print(result.metrics)\n",
    "\n",
    "print(\"\\n--- Agent Evaluation Results DataFrame (Per-Instance Details) ---\")\n",
    "# This DataFrame will show the detailed results for each row in your eval_dataset,\n",
    "# including predicted_trajectory, reference_trajectory, and individual metric scores.\n",
    "print(result.to_dataframe())\n",
    "\n",
    "print(\"\\n--- Evaluation Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
