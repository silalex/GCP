{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e7211a9",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "At a high level, these strategies all involve providing the model clear and specific instructions for what output it should produce. In this lab, you will:\n",
    "\n",
    "- Define the output format & specify constraints\n",
    "- Assign a persona or role\n",
    "- Include examples\n",
    "- Experiment with parameter values\n",
    "- Utilize fallback responses\n",
    "- Add contextual information\n",
    "- Structure prompts with prefixes or tags\n",
    "- Use system instructions\n",
    "- Break down complex tasks\n",
    "- Demonstrate the chain-of-thought\n",
    "- Implement prompt iteration strategies to improve your prompts version by version\n",
    "\n",
    "### Task 1. Initialize Vertex AI in a Colab Enterprise notebook\n",
    "\n",
    "Upgrade the Vertex AI SDK & Restart the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet google-cloud-aiplatform\n",
    "\n",
    "# note: Select Runtime > Restart Session > select Yes to restart the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import cleandoc\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720d1b2",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e2f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"qwiklabs-gcp-03-f2605c9303f1\"  \n",
    "LOCATION = \"us-central1\"\n",
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7ec07",
   "metadata": {},
   "source": [
    "### Task 2. Load a generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2ef308",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-2.0-flash-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78a723",
   "metadata": {},
   "source": [
    "### Task 3. Define the output format & specify constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbda7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store a transcript from a customer's order at a fast food restaurant.\n",
    "\n",
    "transcript = \"\"\"\n",
    "    Speaker 1 (Customer): Hi, can I get a cheeseburger and large fries, please?\n",
    "    Speaker 2 (Restaurant employee): Coming right up! Anything else you'd like to add to your order?\n",
    "    Speaker 1: Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\n",
    "    Speaker 2: No problem, one cheeseburger, one large fries with ketchup on the side, and a small\n",
    "    orange juice. That'll be $5.87. Drive through to the next window please.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following prompt that attempts to understand a customer's order from a conversation in JSON format, but in an unspecific way.\n",
    "\n",
    "response = model.generate_content(f\"\"\"\n",
    "    Extract the transcript to JSON.\n",
    "\n",
    "    {transcript}\n",
    "\"\"\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586eb20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run a version of this prompt with more specific instructions about exactly how you would like your output structured. Notice how the JSON output now reflects the key pieces of information you are interested in to understand the user's order:\n",
    "\n",
    "response = model.generate_content(f\"\"\"\n",
    "    <INSTRUCTIONS>\n",
    "    - Extract the ordered items into JSON.\n",
    "    - Separate drinks from food.\n",
    "    - Include a quantity for each item and a size if specified.\n",
    "    </INSTRUCTIONS>\n",
    "\n",
    "    <TRANSCRIPT>\n",
    "    {transcript}\n",
    "    </TRANSCRIPT>\n",
    "\"\"\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb01b8a3",
   "metadata": {},
   "source": [
    "### Task 4. Assign a persona or role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9044119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try an example within the context of a chat. To start, create a chat session with your Gemini model:\n",
    "chat = model.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask for a response without a persona specified:\n",
    "response = chat.send_message(\n",
    "    \"\"\"\n",
    "    Provide a brief guide to caring for the houseplant monstera deliciosa?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2625d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run a version of this prompt with a role specified. Notice how this output might be more personal and appealing for some users.\n",
    "new_chat = model.start_chat()\n",
    "\n",
    "response = new_chat.send_message(\n",
    "    \"\"\"\n",
    "    You are a houseplant monstera deliciosa. Help the person who\n",
    "    is taking care of you to understand your needs.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fef29b",
   "metadata": {},
   "source": [
    "### Task 5. Include examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34633aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code below in a new cell to see how it interprets the Customer Message and explains its response\n",
    "question = \"\"\"\n",
    "We offer software consulting services. Read a potential\n",
    "customer's message and rank them on a scale of 1 to 3\n",
    "based on whether they seem likely to hire us for our\n",
    "developer services within the next month. Return the likelihood\n",
    "rating labeled as \"Likelihood: SCORE\".\n",
    "Do not include any Markdown styling.\n",
    "\n",
    "1 means they are not likely to hire.\n",
    "2 means they might hire, but they are not likely ready to do\n",
    "so right away.\n",
    "3 means they are looking to start a project soon.\n",
    "\n",
    "Example Message: Hey there I had an idea for an app,\n",
    "and I have no idea what it would cost to build it.\n",
    "Can you give me a rough ballpark?\n",
    "Likelihood: 1\n",
    "\n",
    "Example Message: My department has been using a vendor for\n",
    "our development, and we are interested in exploring other\n",
    "options. Do you have time for a discussion around your\n",
    "services?\n",
    "Likelihood: 2\n",
    "\n",
    "Example Message: I have mockups drawn for an app and a budget\n",
    "allocated. We are interested in moving forward to have a\n",
    "proof of concept built within 2 months, with plans to develop\n",
    "it further in the following quarter.\n",
    "Likelihood: 3\n",
    "\n",
    "Customer Message: Our department needs a custom gen AI solution.\n",
    "We have a budget to explore our idea. Do you have capacity\n",
    "to get started on something soon?\n",
    "Likelihood: \"\"\"\n",
    "\n",
    "response = model.generate_content(question)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1370b3",
   "metadata": {},
   "source": [
    "### Task 6. Experiment with parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da87389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code from the following cell a few times. The temperature and top_p parameters which lead to variety in responses are set to low values, so the output should be the same, or very close to the same, each time.\n",
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    Tell me a joke about frogs.\n",
    "    \"\"\",\n",
    "    generation_config={\"top_p\": .05,\n",
    "                       \"temperature\": 0.05}\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc5455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run this version of the code a few times. You'll see that the higher temperature and top_p parameter values now lead to more varied results. Some of the results, however, may be a little too random and not make very much sense. If you want variety in your responses, you'll need to experiment with parameters to determine the right balance of creativity and reliability.\n",
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    Tell me a joke about frogs.\n",
    "    \"\"\",\n",
    "    generation_config={\"top_p\": .98,\n",
    "                       \"temperature\": 1}\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d1b09",
   "metadata": {},
   "source": [
    "### Task 7. Utilize fallback responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b550e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the following code cells to your notebook and run them to see the model decline to talk about off-topic queries while still answering on-topic queries.\n",
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    Instructions: Answer questions about pottery.\n",
    "    If a user asks about something else, reply with:\n",
    "    Sorry, I only talk about pottery!\n",
    "\n",
    "    User Query: How high can a horse jump?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll also want to test that the model does not reject on-topic questions, so make sure to try out a variety of questions that the model should not decline to answer. Here is one:\n",
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    Instructions: Answer questions about pottery.\n",
    "    If a user asks about something else, reply with:\n",
    "    Sorry, I only talk about pottery!\n",
    "\n",
    "    User Query: What is the difference between ceramic\n",
    "    and porcelain? Please keep your response brief.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82a5a9b",
   "metadata": {},
   "source": [
    "### Task 8. Add contextual information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a4f5c1",
   "metadata": {},
   "source": [
    "# Imagine you work for a grocery store chain and want to provide users a way of finding items in your store easily.\n",
    "\n",
    "# Run the following prompt to inquire about the aisle numbers of some items that can be found in a grocery store.\n",
    "\n",
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    On what aisle numbers can I find the following items?\n",
    "    - paper plates\n",
    "    - mustard\n",
    "    - potatoes\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0fba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run a version of this prompt where you provide it that information in what is called the \"context\" part of the prompt:\n",
    "response = model.generate_content(\"\"\"\n",
    "    Context:\n",
    "    Michael's Grocery Store Aisle Layout:\n",
    "    Aisle 1: Fruits — Apples, bananas,  grapes, oranges, strawberries, avocados, peaches, etc.\n",
    "    Aisle 2: Vegetables — Potatoes, onions, carrots, salad greens, broccoli, peppers, tomatoes, cucumbers, etc.\n",
    "    Aisle 3: Canned Goods — Soup, tuna, fruit, beans, vegetables, pasta sauce, etc.\n",
    "    Aisle 4: Dairy — Butter, cheese, eggs, milk, yogurt, etc.\n",
    "    Aisle 5: Meat— Chicken, beef, pork, sausage, bacon etc.\n",
    "    Aisle 6: Fish & Seafood— Shrimp, crab, cod, tuna, salmon, etc.\n",
    "    Aisle 7: Deli— Cheese, salami, ham, turkey, etc.\n",
    "    Aisle 8: Condiments & Spices— Black pepper, oregano, cinnamon, sugar, olive oil, ketchup, mayonnaise, etc.\n",
    "    Aisle 9: Snacks— Chips, pretzels, popcorn, crackers, nuts, etc.\n",
    "    Aisle 10: Bread & Bakery— Bread, tortillas, pies, muffins, bagels, cookies, etc.\n",
    "    Aisle 11: Beverages— Coffee, teabags, milk, juice, soda, beer, wine, etc.\n",
    "    Aisle 12: Pasta, Rice & Cereal—Oats, granola, brown rice, white rice, macaroni, noodles, etc.\n",
    "    Aisle 13: Baking— Flour, powdered sugar, baking powder, cocoa etc.\n",
    "    Aisle 14: Frozen Foods — Pizza, fish, potatoes, ready meals, ice cream, etc.\n",
    "    Aisle 15: Personal Care— Shampoo, conditioner, deodorant, toothpaste, dental floss, etc.\n",
    "    Aisle 16: Health Care— Saline, band-aid, cleaning alcohol, pain killers, antacids, etc.\n",
    "    Aisle 17: Household & Cleaning Supplies—Laundry detergent, dish soap, dishwashing liquid, paper towels, tissues, trash bags, aluminum foil, zip bags, etc.\n",
    "    Aisle 18: Baby Items— Baby food, diapers, wet wipes, lotion, etc.\n",
    "    Aisle 19: Pet Care— Pet food, kitty litter, chew toys, pet treats, pet shampoo, etc.\n",
    "\n",
    "    Query:\n",
    "    On what aisle numbers can I find the following items?\n",
    "    - paper plates\n",
    "    - mustard\n",
    "    - potatoes\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b5560",
   "metadata": {},
   "source": [
    "### Task 9. Structure prompts with prefixes or tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce1ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how the XML-style tags (like <OBJECTIVE_AND_PERSONA>) divide up sections of the prompt and other prefixes like Name: identify other key pieces of information. This allows for complex structure within a prompt while keeping each section clearly defined.\n",
    "\n",
    "prompt = \"\"\"\n",
    "  <OBJECTIVE_AND_PERSONA>\n",
    "  You are a dating matchmaker.\n",
    "  Your task is to identify common topics or interests between\n",
    "  the USER_ATTRIBUTES and POTENTIAL_MATCH options and present them\n",
    "  as a fun and meaningful potential matches.\n",
    "  </OBJECTIVE_AND_PERSONA>\n",
    "\n",
    "  <INSTRUCTIONS>\n",
    "  To complete the task, you need to follow these steps:\n",
    "  1. Identify matching or complimentary elements from the\n",
    "     USER_ATTRIBUTES and the POTENTIAL_MATCH options.\n",
    "  2. Pick the POTENTIAL_MATCH that represents the best match to the USER_ATTRIBUTES\n",
    "  3. Describe that POTENTIAL_MATCH like an encouraging friend who has\n",
    "     found a good dating prospect for a friend.\n",
    "  4. Don't insult the user or potential matches.\n",
    "  5. Only mention the best match. Don't mention the other potential matches.\n",
    "  </INSTRUCTIONS>\n",
    "\n",
    "  <CONTEXT>\n",
    "  <USER_ATTRIBUTES>\n",
    "  Name: Allison\n",
    "  I like to go to classical music concerts and the theatre.\n",
    "  I like to swim.\n",
    "  I don't like sports.\n",
    "  My favorite cuisines are Italian and ramen. Anything with noodles!\n",
    "  </USER_ATTRIBUTES>\n",
    "\n",
    "  <POTENTIAL_MATCH 1>\n",
    "  Name: Jason\n",
    "  I'm very into sports.\n",
    "  My favorite team is the Detroit Lions.\n",
    "  I like baked potatoes.\n",
    "  </POTENTIAL_MATCH 1>\n",
    "\n",
    "  <POTENTIAL_MATCH 2>\n",
    "  Name: Felix\n",
    "  I'm very into Beethoven.\n",
    "  I like German food. I make a good spaetzle, which is like a German pasta.\n",
    "  I used to play water polo and still love going to the beach.\n",
    "  </POTENTIAL_MATCH 2>\n",
    "  </CONTEXT>\n",
    "\n",
    "  <OUTPUT_FORMAT>\n",
    "  Format results in Markdown.\n",
    "  </OUTPUT_FORMAT>\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa2521f",
   "metadata": {},
   "source": [
    "### Task 10. Use system instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feef296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can include prompt components like those you've explored above in each call to a model, or you can pass them to the model upon instantiation as system instructions.\n",
    "\n",
    "# Paste the code below into a new code cell and run it.\n",
    "\n",
    "# Notice how the prompt passed to the generate_content() function doesn't mention music at all, but the model still responds based on the persona & instructions passed to it as a system_instruction.\n",
    "\n",
    "# Note: Content provided as a system instructions are billed as though they were passed in as part of each prompt at generation time.\n",
    "\n",
    "system_instructions = \"\"\"\n",
    "    You will respond as a music historian,\n",
    "    demonstrating comprehensive knowledge\n",
    "    across diverse musical genres and providing\n",
    "    relevant examples. Your tone will be upbeat\n",
    "    and enthusiastic, spreading the joy of music.\n",
    "    If a question is not related to music, the\n",
    "    response should be, 'That is beyond my knowledge.'\n",
    "\"\"\"\n",
    "\n",
    "music_model = GenerativeModel(\"gemini-1.5-pro\",\n",
    "                    system_instruction=system_instructions)\n",
    "\n",
    "response = music_model.generate_content(\n",
    "    \"\"\"\n",
    "    Who is worth studying?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6672c1b",
   "metadata": {},
   "source": [
    "### Task 11. Demonstrate Chain-of-Thought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eb1337",
   "metadata": {},
   "source": [
    "# Large language models predict what language should follow another language, but they cannot think through cause and effect in the world outside of language. For tasks that require more reasoning, it can help to guide the model through expressing intermediate logical steps in language.\n",
    "\n",
    "# Large Language Models, especially Gemini, have gotten much better at reasoning on their own. But they can sometimes still use guidance to assist in laying out one logical step at a time.\n",
    "\n",
    "# Notice in the code cell below that you will pass a generation_config parameter to the generate_content() function. It is a best practice to set the temperature to 0 for math and logical problems where you would like a precisely correct answer.\n",
    "\n",
    "question = \"\"\"\n",
    "Instructions:\n",
    "Use the context and make any updates needed in the scenario to answer the question.\n",
    "\n",
    "Context:\n",
    "A high efficiency factory produces 100 units per day.\n",
    "A medium efficiency factory produces 60 units per day.\n",
    "A low efficiency factory produces 30 units per day.\n",
    "\n",
    "Megacorp owns 5 factories. 3 are high efficiency, 2 are low efficiency.\n",
    "\n",
    "<EXAMPLE SCENARIO>\n",
    "Scenario:\n",
    "Tomorrow Megacorp will have to shut down one high efficiency factory.\n",
    "It will add two rented medium efficiency factories to make up production.\n",
    "\n",
    "Question:\n",
    "How many units can they produce today? How many tomorrow?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Today's Production:\n",
    "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
    "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
    "* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n",
    "\n",
    "Tomorrow's Production:\n",
    "* High efficiency factories: 2 factories * 100 units/day/factory = 200 units/day\n",
    "* Medium efficiency factories: 2 factories * 60 units/day/factory = 120 units/day\n",
    "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
    "* **Total production today: 300 units/day + 60 units/day = 380 units/day**\n",
    "</EXAMPLE SCENARIO>\n",
    "\n",
    "<SCENARIO>\n",
    "Scenario:\n",
    "Tomorrow Megacorp will reconfigure a low efficiency factory up to medium efficiency.\n",
    "And the remaining low efficiency factory has an outage that cuts output in half.\n",
    "\n",
    "Question:\n",
    "How many units can they produce today? How many tomorrow?\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "response = model.generate_content(question,\n",
    "                                  generation_config={\"temperature\": 0})\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
