{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e7211a9",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "At a high level, these strategies all involve providing the model clear and specific instructions for what output it should produce. In this lab, you will:\n",
    "\n",
    "- Define the output format & specify constraints\n",
    "- Assign a persona or role\n",
    "- Include examples\n",
    "- Experiment with parameter values\n",
    "- Utilize fallback responses\n",
    "- Add contextual information\n",
    "- Structure prompts with prefixes or tags\n",
    "- Use system instructions\n",
    "- Break down complex tasks\n",
    "- Demonstrate the chain-of-thought\n",
    "- Implement prompt iteration strategies to improve your prompts version by version\n",
    "\n",
    "### Task 1. Initialize Vertex AI in a Colab Enterprise notebook\n",
    "\n",
    "Upgrade the Vertex AI SDK & Restart the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet google-cloud-aiplatform\n",
    "\n",
    "# note: Select Runtime > Restart Session > select Yes to restart the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import cleandoc\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720d1b2",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e2f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"qwiklabs-gcp-03-f2605c9303f1\"  \n",
    "LOCATION = \"us-central1\"\n",
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7ec07",
   "metadata": {},
   "source": [
    "### Task 2. Load a generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2ef308",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-2.0-flash-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78a723",
   "metadata": {},
   "source": [
    "### Task 3. Define the output format & specify constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbda7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store a transcript from a customer's order at a fast food restaurant.\n",
    "\n",
    "transcript = \"\"\"\n",
    "    Speaker 1 (Customer): Hi, can I get a cheeseburger and large fries, please?\n",
    "    Speaker 2 (Restaurant employee): Coming right up! Anything else you'd like to add to your order?\n",
    "    Speaker 1: Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\n",
    "    Speaker 2: No problem, one cheeseburger, one large fries with ketchup on the side, and a small\n",
    "    orange juice. That'll be $5.87. Drive through to the next window please.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following prompt that attempts to understand a customer's order from a conversation in JSON format, but in an unspecific way.\n",
    "\n",
    "response = model.generate_content(f\"\"\"\n",
    "    Extract the transcript to JSON.\n",
    "\n",
    "    {transcript}\n",
    "\"\"\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586eb20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run a version of this prompt with more specific instructions about exactly how you would like your output structured. Notice how the JSON output now reflects the key pieces of information you are interested in to understand the user's order:\n",
    "\n",
    "response = model.generate_content(f\"\"\"\n",
    "    <INSTRUCTIONS>\n",
    "    - Extract the ordered items into JSON.\n",
    "    - Separate drinks from food.\n",
    "    - Include a quantity for each item and a size if specified.\n",
    "    </INSTRUCTIONS>\n",
    "\n",
    "    <TRANSCRIPT>\n",
    "    {transcript}\n",
    "    </TRANSCRIPT>\n",
    "\"\"\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb01b8a3",
   "metadata": {},
   "source": [
    "### Task 4. Assign a persona or role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9044119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try an example within the context of a chat. To start, create a chat session with your Gemini model:\n",
    "chat = model.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask for a response without a persona specified:\n",
    "response = chat.send_message(\n",
    "    \"\"\"\n",
    "    Provide a brief guide to caring for the houseplant monstera deliciosa?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2625d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run a version of this prompt with a role specified. Notice how this output might be more personal and appealing for some users.\n",
    "new_chat = model.start_chat()\n",
    "\n",
    "response = new_chat.send_message(\n",
    "    \"\"\"\n",
    "    You are a houseplant monstera deliciosa. Help the person who\n",
    "    is taking care of you to understand your needs.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fef29b",
   "metadata": {},
   "source": [
    "### Task 5. Include examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34633aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code below in a new cell to see how it interprets the Customer Message and explains its response\n",
    "question = \"\"\"\n",
    "We offer software consulting services. Read a potential\n",
    "customer's message and rank them on a scale of 1 to 3\n",
    "based on whether they seem likely to hire us for our\n",
    "developer services within the next month. Return the likelihood\n",
    "rating labeled as \"Likelihood: SCORE\".\n",
    "Do not include any Markdown styling.\n",
    "\n",
    "1 means they are not likely to hire.\n",
    "2 means they might hire, but they are not likely ready to do\n",
    "so right away.\n",
    "3 means they are looking to start a project soon.\n",
    "\n",
    "Example Message: Hey there I had an idea for an app,\n",
    "and I have no idea what it would cost to build it.\n",
    "Can you give me a rough ballpark?\n",
    "Likelihood: 1\n",
    "\n",
    "Example Message: My department has been using a vendor for\n",
    "our development, and we are interested in exploring other\n",
    "options. Do you have time for a discussion around your\n",
    "services?\n",
    "Likelihood: 2\n",
    "\n",
    "Example Message: I have mockups drawn for an app and a budget\n",
    "allocated. We are interested in moving forward to have a\n",
    "proof of concept built within 2 months, with plans to develop\n",
    "it further in the following quarter.\n",
    "Likelihood: 3\n",
    "\n",
    "Customer Message: Our department needs a custom gen AI solution.\n",
    "We have a budget to explore our idea. Do you have capacity\n",
    "to get started on something soon?\n",
    "Likelihood: \"\"\"\n",
    "\n",
    "response = model.generate_content(question)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1370b3",
   "metadata": {},
   "source": [
    "### Task 6. Experiment with parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da87389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code from the following cell a few times. The temperature and top_p parameters which lead to variety in responses are set to low values, so the output should be the same, or very close to the same, each time.\n",
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    Tell me a joke about frogs.\n",
    "    \"\"\",\n",
    "    generation_config={\"top_p\": .05,\n",
    "                       \"temperature\": 0.05}\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc5455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run this version of the code a few times. You'll see that the higher temperature and top_p parameter values now lead to more varied results. Some of the results, however, may be a little too random and not make very much sense. If you want variety in your responses, you'll need to experiment with parameters to determine the right balance of creativity and reliability.\n",
    "response = model.generate_content(\n",
    "    \"\"\"\n",
    "    Tell me a joke about frogs.\n",
    "    \"\"\",\n",
    "    generation_config={\"top_p\": .98,\n",
    "                       \"temperature\": 1}\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b550e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
