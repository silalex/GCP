{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e7211a9",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "At a high level, these strategies all involve providing the model clear and specific instructions for what output it should produce. In this lab, you will:\n",
    "\n",
    "- Define the output format & specify constraints\n",
    "- Assign a persona or role\n",
    "- Include examples\n",
    "- Experiment with parameter values\n",
    "- Utilize fallback responses\n",
    "- Add contextual information\n",
    "- Structure prompts with prefixes or tags\n",
    "- Use system instructions\n",
    "- Break down complex tasks\n",
    "- Demonstrate the chain-of-thought\n",
    "- Implement prompt iteration strategies to improve your prompts version by version\n",
    "\n",
    "# Task 1. Initialize Vertex AI in a Colab Enterprise notebook\n",
    "\n",
    "### Upgrade the Vertex AI SDK & Restart the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet google-cloud-aiplatform\n",
    "\n",
    "# note: Select Runtime > Restart Session > select Yes to restart the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import cleandoc\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720d1b2",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e2f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"qwiklabs-gcp-03-f2605c9303f1\"  \n",
    "LOCATION = \"us-central1\"\n",
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7ec07",
   "metadata": {},
   "source": [
    "# Task 2. Load a generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2ef308",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-2.0-flash-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78a723",
   "metadata": {},
   "source": [
    "# Task 3. Define the output format & specify constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbda7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store a transcript from a customer's order at a fast food restaurant.\n",
    "\n",
    "transcript = \"\"\"\n",
    "    Speaker 1 (Customer): Hi, can I get a cheeseburger and large fries, please?\n",
    "    Speaker 2 (Restaurant employee): Coming right up! Anything else you'd like to add to your order?\n",
    "    Speaker 1: Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\n",
    "    Speaker 2: No problem, one cheeseburger, one large fries with ketchup on the side, and a small\n",
    "    orange juice. That'll be $5.87. Drive through to the next window please.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following prompt that attempts to understand a customer's order from a conversation in JSON format, but in an unspecific way.\n",
    "\n",
    "response = model.generate_content(f\"\"\"\n",
    "    Extract the transcript to JSON.\n",
    "\n",
    "    {transcript}\n",
    "\"\"\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586eb20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run a version of this prompt with more specific instructions about exactly how you would like your output structured. Notice how the JSON output now reflects the key pieces of information you are interested in to understand the user's order:\n",
    "\n",
    "response = model.generate_content(f\"\"\"\n",
    "    <INSTRUCTIONS>\n",
    "    - Extract the ordered items into JSON.\n",
    "    - Separate drinks from food.\n",
    "    - Include a quantity for each item and a size if specified.\n",
    "    </INSTRUCTIONS>\n",
    "\n",
    "    <TRANSCRIPT>\n",
    "    {transcript}\n",
    "    </TRANSCRIPT>\n",
    "\"\"\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb01b8a3",
   "metadata": {},
   "source": [
    "# Task 4. Assign a persona or role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9044119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try an example within the context of a chat. To start, create a chat session with your Gemini model:\n",
    "chat = model.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask for a response without a persona specified:\n",
    "response = chat.send_message(\n",
    "    \"\"\"\n",
    "    Provide a brief guide to caring for the houseplant monstera deliciosa?\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2625d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run a version of this prompt with a role specified. Notice how this output might be more personal and appealing for some users.\n",
    "new_chat = model.start_chat()\n",
    "\n",
    "response = new_chat.send_message(\n",
    "    \"\"\"\n",
    "    You are a houseplant monstera deliciosa. Help the person who\n",
    "    is taking care of you to understand your needs.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
