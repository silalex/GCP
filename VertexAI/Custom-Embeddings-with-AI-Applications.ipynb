{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff25c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -q --upgrade --user google-cloud-aiplatform google-cloud-discoveryengine google-cloud-storage google-cloud-bigquery[pandas] google-cloud-bigquery-storage pandas ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9d762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f16b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e1541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code imports various python libraries and modules to perform tasks related to Google Cloud Platform (GCP) services, specifically related to Vertex AI, BigQuery, Cloud Storage, and Discovery Engine.\n",
    "\n",
    "from typing import List\n",
    "import requests\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.api_core.exceptions import GoogleAPICallError\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import discoveryengine_v1alpha as discoveryengine\n",
    "from google.cloud import storage\n",
    "\n",
    "from tqdm import tqdm  # to show a progress bar\n",
    "\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code sets up project information and initializes the Vertex AI SDK for a Google Cloud Platform (GCP) project.\n",
    "\n",
    "# Define project information for Vertex AI\n",
    "PROJECT_ID = \"qwiklabs-gcp-00-a9d5023659fd\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd58e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code snippet in the next cell, that connects to Google BigQuery, executes a SQL query to retrieve information from the Stack Overflow dataset, loads the results into a Pandas DataFrame, and then performs some data manipulation.\n",
    "\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "  DISTINCT\n",
    "  q.id,\n",
    "  q.title,\n",
    "  q.body,\n",
    "  q.answer_count,\n",
    "  q.comment_count,\n",
    "  q.creation_date,\n",
    "  q.last_activity_date,\n",
    "  q.score,\n",
    "  q.tags,\n",
    "  q.view_count\n",
    "FROM\n",
    "  `bigquery-public-data.stackoverflow.posts_questions` AS q\n",
    "WHERE\n",
    "  q.score > 0\n",
    "ORDER BY\n",
    "  q.view_count DESC\n",
    "LIMIT\n",
    "  500;\n",
    "\"\"\"\n",
    "\n",
    "# Load the BQ Table into a Pandas Dataframe\n",
    "df = bq_client.query(query).result().to_dataframe()\n",
    "\n",
    "# Convert ID to String\n",
    "df[\"id\"] = df[\"id\"].apply(str)\n",
    "\n",
    "# examine the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f653ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output 2 columns from the dataset:\n",
    "df[['title', 'body']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text embeddings model\n",
    "model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb1ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for a list of texts\n",
    "def get_embeddings_wrapper(texts, batch_size: int = 50) -> List:\n",
    "    embs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        # Create embeddings optimized for document retrieval\n",
    "        # (supported in textembedding-gecko@002 and later)\n",
    "        result = model.get_embeddings(\n",
    "            [\n",
    "                TextEmbeddingInput(text=text, task_type=\"RETRIEVAL_DOCUMENT\")\n",
    "                for text in texts[i : i + batch_size]\n",
    "            ]\n",
    "        )\n",
    "        embs.extend([e.values for e in result])\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# his code snippet modifies the previously loaded DataFrame (df) by combining the title and body columns into a new title_body column\n",
    "\n",
    "df[\"title_body\"] = df[\"title\"] + \"\\n\" + df[\"body\"]\n",
    "\n",
    "df = df.assign(embedding=get_embeddings_wrapper(df.title_body))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba5aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"qwiklabs-gcp-00-a9d5023659fd\"\n",
    "BUCKET_URI = \"gs://qwiklabs-gcp-00-a9d5023659fd\"\n",
    "REGION = \"us-east4\"\n",
    "PROJECT_ID = \"qwiklabs-gcp-00-a9d5023659fd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create the Google Cloud Storage bucket.\n",
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8890e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories within the bucket.\n",
    " \n",
    "%%bash\n",
    "\n",
    "# Set your Google Cloud Storage bucket name\n",
    "BUCKET_NAME=\"gs://qwiklabs-gcp-00-a9d5023659fd\"\n",
    "\n",
    "# Array of top-level directory names you want to create\n",
    "TOP_LEVEL_DIRECTORIES=(\"embeddings-stackoverflow\")\n",
    "\n",
    "# Loop through the top-level array and create directories\n",
    "for TOP_LEVEL_DIRECTORY in \"${TOP_LEVEL_DIRECTORIES[@]}\"; do\n",
    "  gsutil -m cp -r /dev/null \"$BUCKET_NAME/$TOP_LEVEL_DIRECTORY/\"\n",
    "\n",
    "  # Array of subdirectory names you want to create inside the top-level directory\n",
    "  SUBDIRECTORIES=(\"html\")\n",
    "\n",
    "  # Loop through the subdirectories array and create subdirectories inside the top-level directory\n",
    "  for SUBDIRECTORY in \"${SUBDIRECTORIES[@]}\"; do\n",
    "    gsutil -m cp -r /dev/null \"$BUCKET_NAME/$TOP_LEVEL_DIRECTORY/$SUBDIRECTORY/\"\n",
    "  done\n",
    "done\n",
    "\n",
    "echo \"Directories created successfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4edbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run the following code snippet in the next cell, to define a Python function named scrape_question that performs the following tasks:\n",
    " - It sends an HTTP GET request to a specified URL (question_url) to scrape the content of a question page.\n",
    " - If the request is successful (HTTP status code 200) and the response contains content, it uploads the HTML content of the question page to Google Cloud Storage (GCS).\n",
    " - The GCS URI (Uniform Resource Identifier) of the uploaded HTML file is returned.\n",
    "'''\n",
    "\n",
    "JSONL_MIME_TYPE = \"application/jsonl\"\n",
    "HTML_MIME_TYPE = \"text/html\"\n",
    "\n",
    "BUCKET_NAME = \"qwiklabs-gcp-00-a9d5023659fd\"\n",
    "DIRECTORY = \"embeddings-stackoverflow\"\n",
    "BLOB_PREFIX = f\"{DIRECTORY}/html/\"\n",
    "\n",
    "GCS_URI_PREFIX = f\"gs://{BUCKET_NAME}/{BLOB_PREFIX}\"\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "\n",
    "def scrape_question(question_url: str) -> str:\n",
    "    response = requests.get(question_url)\n",
    "\n",
    "    if response.status_code != 200 or not response.content:\n",
    "        print(f\"URL: {question_url} Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Scraping {question_url}\")\n",
    "\n",
    "    link_title = response.url.split(\"/\")[-1] + \".html\"\n",
    "    gcs_uri = f\"{GCS_URI_PREFIX}{link_title}\"\n",
    "\n",
    "    # Upload HTML to Google Cloud Storage\n",
    "    blob = bucket.blob(f\"{BLOB_PREFIX}{link_title}\")\n",
    "    blob.upload_from_string(response.content, content_type=HTML_MIME_TYPE)\n",
    "    time.sleep(1)\n",
    "    return gcs_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d65aa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code snippet has two main parts: it constructs URLs for Stack Overflow questions based on their IDs and then scrapes the HTML content from each of these URLs before uploading it to Google Cloud Storage (GCS). Run the following code snippet in the next cell.\n",
    "'''\n",
    "\n",
    "# Get the published URL from the ID\n",
    "QUESTION_BASE_URL = \"https://stackoverflow.com/questions/\"\n",
    "df[\"question_url\"] = df[\"id\"].apply(lambda x: f\"{QUESTION_BASE_URL}{x}\")\n",
    "\n",
    "# Scrape HTML from stackoverflow.com and upload to GCS\n",
    "df[\"gcs_uri\"] = df[\"question_url\"].apply(scrape_question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the next cell, restructure the embeddings data to JSONL to follow the Vertex AI Search format (Unstructured with Metadata). This format is required to use custom embeddings.\n",
    "EMBEDDINGS_FIELD_NAME = \"embedding_vector\"\n",
    "\n",
    "def format_row(row):\n",
    "    return {\n",
    "        \"id\": row[\"id\"],\n",
    "        \"content\": {\"mimeType\": HTML_MIME_TYPE, \"uri\": row[\"gcs_uri\"]},\n",
    "        \"structData\": {\n",
    "            EMBEDDINGS_FIELD_NAME: row[\"embedding\"],\n",
    "            \"title\": row[\"title\"],\n",
    "            \"body\": row[\"body\"],\n",
    "            \"question_url\": row[\"question_url\"],\n",
    "            \"answer_count\": row[\"answer_count\"],\n",
    "            \"creation_date\": row[\"creation_date\"],\n",
    "            \"score\": row[\"score\"],\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "vais_embeddings = (\n",
    "    df.apply(format_row, axis=1)\n",
    "    .to_json(orient=\"records\", lines=True, force_ascii=False)\n",
    "    .replace(\"\\/\", \"/\")  # To prevent escaping the / characters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e30ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the next cell, upload the JSONL file to Google Cloud Storage.\n",
    "jsonl_filename = f\"{DIRECTORY}/vais_embeddings.jsonl\"\n",
    "embeddings_file = f\"gs://{BUCKET_NAME}/{jsonl_filename}\"\n",
    "\n",
    "blob = bucket.blob(jsonl_filename)\n",
    "blob.upload_from_string(vais_embeddings, content_type=JSONL_MIME_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9342f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the next cell, set up client options for interacting with the Google Cloud Vertex AI Discovery Engine service. It specifies the API endpoint based on the provided DATA_STORE_LOCATION.\n",
    "\n",
    "DATA_STORE_LOCATION = \"global\"\n",
    "\n",
    "client_options = (\n",
    "    ClientOptions(api_endpoint=f\"{DATA_STORE_LOCATION}-discoveryengine.googleapis.com\")\n",
    "    if DATA_STORE_LOCATION != \"global\"\n",
    "    else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006950d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the next cell, define several functions that interact with the Google Cloud Vertex AI Discovery Engine service. These functions are responsible for creating a data store, updating its schema, importing documents, and creating a search engine.\n",
    "\n",
    "def create_data_store(\n",
    "    project_id: str, location: str, data_store_name: str, data_store_id: str\n",
    "):\n",
    "    # Create a client\n",
    "    client = discoveryengine.DataStoreServiceClient(client_options=client_options)\n",
    "\n",
    "    # Initialize request argument(s)\n",
    "    data_store = discoveryengine.DataStore(\n",
    "        display_name=data_store_name,\n",
    "        industry_vertical=\"GENERIC\",\n",
    "        content_config=\"CONTENT_REQUIRED\",\n",
    "        solution_types=[\"SOLUTION_TYPE_SEARCH\"],\n",
    "    )\n",
    "\n",
    "    request = discoveryengine.CreateDataStoreRequest(\n",
    "        parent=discoveryengine.DataStoreServiceClient.collection_path(\n",
    "            project_id, location, \"default_collection\"\n",
    "        ),\n",
    "        data_store=data_store,\n",
    "        data_store_id=data_store_id,\n",
    "    )\n",
    "    operation = client.create_data_store(request=request)\n",
    "\n",
    "    try:\n",
    "        operation.result()\n",
    "    except GoogleAPICallError:\n",
    "        pass\n",
    "\n",
    "\n",
    "def update_schema(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    data_store_id: str,\n",
    "):\n",
    "    client = discoveryengine.SchemaServiceClient(client_options=client_options)\n",
    "\n",
    "    schema = discoveryengine.Schema(\n",
    "        name=client.schema_path(project_id, location, data_store_id, \"default_schema\"),\n",
    "        struct_schema={\n",
    "            \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                EMBEDDINGS_FIELD_NAME: {\n",
    "                    \"type\": \"array\",\n",
    "                    \"keyPropertyMapping\": \"embedding_vector\",\n",
    "                    \"dimension\": 768,\n",
    "                    \"items\": {\"type\": \"number\"},\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    operation = client.update_schema(\n",
    "        request=discoveryengine.UpdateSchemaRequest(schema=schema)\n",
    "    )\n",
    "\n",
    "    print(\"Waiting for operation to complete...\")\n",
    "\n",
    "    response = operation.result()\n",
    "\n",
    "    # Handle the response\n",
    "    print(response)\n",
    "\n",
    "\n",
    "def import_documents(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    data_store_id: str,\n",
    "    gcs_uri: str,\n",
    "):\n",
    "    client = discoveryengine.DocumentServiceClient(client_options=client_options)\n",
    "\n",
    "    # The full resource name of the search engine branch.\n",
    "    # e.g. projects/{project}/locations/{location}/dataStores/{data_store_id}/branches/{branch}\n",
    "    parent = client.branch_path(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        data_store=data_store_id,\n",
    "        branch=\"default_branch\",\n",
    "    )\n",
    "\n",
    "    request = discoveryengine.ImportDocumentsRequest(\n",
    "        parent=parent,\n",
    "        gcs_source=discoveryengine.GcsSource(input_uris=[gcs_uri]),\n",
    "        # Options: `FULL`, `INCREMENTAL`\n",
    "        reconciliation_mode=discoveryengine.ImportDocumentsRequest.ReconciliationMode.FULL,\n",
    "    )\n",
    "\n",
    "    # Make the request\n",
    "    operation = client.import_documents(request=request)\n",
    "\n",
    "\n",
    "def create_engine(\n",
    "    project_id: str, location: str, data_store_name: str, data_store_id: str\n",
    "):\n",
    "    client = discoveryengine.EngineServiceClient(client_options=client_options)\n",
    "\n",
    "    # Initialize request argument(s)\n",
    "    config = discoveryengine.Engine.SearchEngineConfig(\n",
    "        search_tier=\"SEARCH_TIER_ENTERPRISE\", search_add_ons=[\"SEARCH_ADD_ON_LLM\"]\n",
    "    )\n",
    "\n",
    "    engine = discoveryengine.Engine(\n",
    "        display_name=data_store_name,\n",
    "        solution_type=\"SOLUTION_TYPE_SEARCH\",\n",
    "        industry_vertical=\"GENERIC\",\n",
    "        data_store_ids=[data_store_id],\n",
    "        search_engine_config=config,\n",
    "    )\n",
    "\n",
    "    request = discoveryengine.CreateEngineRequest(\n",
    "        parent=discoveryengine.DataStoreServiceClient.collection_path(\n",
    "            project_id, location, \"default_collection\"\n",
    "        ),\n",
    "        engine=engine,\n",
    "        engine_id=engine.display_name,\n",
    "    )\n",
    "\n",
    "    # Make the request\n",
    "    operation = client.create_engine(request=request)\n",
    "    response = operation.result(timeout=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e956512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the project related variables\n",
    "\n",
    "DATA_STORE_NAME = \"stackoverflow-embeddings\"\n",
    "DATA_STORE_ID = f\"{DATA_STORE_NAME}-id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf01445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Data Store\n",
    "create_data_store(PROJECT_ID, DATA_STORE_LOCATION, DATA_STORE_NAME, DATA_STORE_ID)\n",
    "\n",
    "# Update the Data Store Schema for embeddings\n",
    "update_schema(PROJECT_ID, DATA_STORE_LOCATION, DATA_STORE_ID)\n",
    "\n",
    "# Import the embeddings JSONL file\n",
    "import_documents(PROJECT_ID, DATA_STORE_LOCATION, DATA_STORE_ID, embeddings_file)\n",
    "\n",
    "# Create a Search App and attach the Data Store\n",
    "create_engine(PROJECT_ID, DATA_STORE_LOCATION, DATA_STORE_NAME, DATA_STORE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2067179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we need to set the embedding specification for the data store. We will set the same specifications for all search requests: 0.5 * relevance_score\n",
    "\n",
    "# Run the following code snippet in the next cell, that retrieves an access token using gcloud auth print-access-token, and then it sends a PATCH request to update the serving configuration of the search application in Google Cloud Vertex AI Discovery Engine. The request includes the embedding configuration and a ranking expression, and the server's response is printed.\n",
    "\n",
    "access_token = (\n",
    "    subprocess.check_output([\"gcloud\", \"auth\", \"print-access-token\"])\n",
    "    .decode(\"utf-8\")\n",
    "    .strip()\n",
    ")\n",
    "\n",
    "response = requests.patch(\n",
    "    url=f\"https://discoveryengine.googleapis.com/v1alpha/projects/{PROJECT_ID}/locations/{DATA_STORE_LOCATION}/collections/default_collection/dataStores/{DATA_STORE_ID}/servingConfigs/default_search?updateMask=embeddingConfig,rankingExpression\",\n",
    "    headers={\n",
    "        \"Authorization\": f\"Bearer {access_token}\",\n",
    "        \"Content-Type\": \"application/json; charset=utf-8\",\n",
    "        \"X-Goog-User-Project\": PROJECT_ID,\n",
    "    },\n",
    "    json={\n",
    "        \"name\": f\"projects/{PROJECT_ID}/locations/{DATA_STORE_LOCATION}/collections/default_collection/dataStores/{DATA_STORE_ID}/servingConfigs/default_search\",\n",
    "        \"embeddingConfig\": {\"fieldPath\": EMBEDDINGS_FIELD_NAME},\n",
    "        \"ranking_expression\": \"0.5 * relevance_score\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code snippet in the next cell, to define a function named search_data_store that performs a search operation on a Google Cloud Vertex AI Discovery Engine data store.\n",
    "\n",
    "def search_data_store(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    data_store_id: str,\n",
    "    search_query: str,\n",
    ") -> List[discoveryengine.SearchResponse]:\n",
    "    # Create a client\n",
    "    client = discoveryengine.SearchServiceClient(client_options=client_options)\n",
    "\n",
    "    # The full resource name of the search engine serving config\n",
    "    # e.g. projects/{project_id}/locations/{location}/dataStores/{data_store_id}/servingConfigs/{serving_config_id}\n",
    "    serving_config = client.serving_config_path(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        data_store=data_store_id,\n",
    "        serving_config=\"default_config\",\n",
    "    )\n",
    "\n",
    "    # Optional: Configuration options for search\n",
    "    # Refer to the `ContentSearchSpec` reference for all supported fields:\n",
    "    # https://cloud.google.com/python/docs/reference/discoveryengine/latest/google.cloud.discoveryengine_v1.types.SearchRequest.ContentSearchSpec\n",
    "    content_search_spec = discoveryengine.SearchRequest.ContentSearchSpec(\n",
    "        # For information about snippets, refer to:\n",
    "        # https://cloud.google.com/generative-ai-app-builder/docs/snippets\n",
    "        snippet_spec=discoveryengine.SearchRequest.ContentSearchSpec.SnippetSpec(\n",
    "            return_snippet=True\n",
    "        ),\n",
    "        # For information about search summaries, refer to:\n",
    "        # https://cloud.google.com/generative-ai-app-builder/docs/get-search-summaries\n",
    "        summary_spec=discoveryengine.SearchRequest.ContentSearchSpec.SummarySpec(\n",
    "            summary_result_count=5,\n",
    "            include_citations=True,\n",
    "            ignore_adversarial_query=True,\n",
    "            ignore_non_summary_seeking_query=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Refer to the `SearchRequest` reference for all supported fields:\n",
    "    # https://cloud.google.com/python/docs/reference/discoveryengine/latest/google.cloud.discoveryengine_v1.types.SearchRequest\n",
    "    request = discoveryengine.SearchRequest(\n",
    "        serving_config=serving_config,\n",
    "        query=search_query,\n",
    "        page_size=10,\n",
    "        content_search_spec=content_search_spec,\n",
    "        query_expansion_spec=discoveryengine.SearchRequest.QueryExpansionSpec(\n",
    "            condition=discoveryengine.SearchRequest.QueryExpansionSpec.Condition.AUTO,\n",
    "        ),\n",
    "        spell_correction_spec=discoveryengine.SearchRequest.SpellCorrectionSpec(\n",
    "            mode=discoveryengine.SearchRequest.SpellCorrectionSpec.Mode.AUTO\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    response = client.search(request)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5347e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code snippet in the next cell, to perform a search operation on a Google Cloud Vertex AI Discovery Engine data store using a specified search query and prints the summary text of the search response.\n",
    "\n",
    "search_query = \"How do I create an array in Java?\"\n",
    "\n",
    "response = search_data_store(\n",
    "    PROJECT_ID, DATA_STORE_LOCATION, DATA_STORE_ID, search_query\n",
    ")\n",
    "\n",
    "print(f\"Summary: {response.summary.summary_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
