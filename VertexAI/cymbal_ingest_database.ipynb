{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Leverage Firestore as a Vector Store (GENAI109)\n",
        "\n",
        "## Overview\n",
        "In this lab, you'll build a real-world Generative AI Question-and-Answer (Q&A) solution using a Retrieval-Augmented Generation (RAG) framework. You will use Firestore as a vector database to store document chunks and embeddings generated with Vertex AI. By querying Firestore with user input, you'll retrieve relevant information, and use Gemini 2.0 Flash to generate natural language answers.\n",
        "\n",
        "Note: To avoid confusion between your professional Google Identity and any other temporary Qwiklabs student accounts, it is strongly recommended that you utilize a new Incognito window for the Google Cloud console and Google Drive tabs you will use in this lab. To do this easily in Chrome, after starting the lab, right-click on the “Open Google Cloud console” button and select “Open link in incognito window”.\n",
        "\n",
        "## Objectives\n",
        "In this lab, you learn how to:\n",
        " - Load and process a document into semantically meaningful chunks\n",
        " - Generate vector embeddings using Vertex AI\n",
        " - Store chunks and embeddings in Firestore for vector search\n",
        " - Query Firestore using user input and retrieve relevant chunks\n",
        " - Use Gemini 2.0 Flash to generate natural language answers in a RAG pipeline"
      ],
      "metadata": {
        "id": "0SuqHQpklASO"
      },
      "id": "0SuqHQpklASO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1. Create a Colab Enterprise Notebook\n",
        "\n",
        "### Upgrade the Vertex AI SDK & Restart the Kernel\n",
        "\n",
        "5. Paste the following code into the top cell and run it with Shift + Return. If you don’t already have an active notebook runtime, running a cell in a Colab Enterprise notebook will trigger it to create one for you and connect the notebook to it. When a runtime is allocated for the first time, you may be presented with a pop-up window to authorize the environment to act as your Qwiklabs student account."
      ],
      "metadata": {
        "id": "1guGXiablTF3"
      },
      "id": "1guGXiablTF3"
    },
    {
      "cell_type": "code",
      "id": "9s8i9FwMKLkwHe4GogZNTkcg",
      "metadata": {
        "tags": [],
        "id": "9s8i9FwMKLkwHe4GogZNTkcg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6374e39-1ec0-4e5a-e191-a04a4ae2f9ea"
      },
      "source": [
        "!pip install --quiet --upgrade google-cloud-logging google_cloud_firestore google_cloud_aiplatform langchain langchain-google-vertexai langchain_community langchain_experimental pymupdf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.5/229.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.7/102.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.9/438.9 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.6/364.6 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Restart the runtime: select **Runtime > Restart Session > select Yes**. The runtime will restart, indicated by clearing the green checkmark and the cell run order integer next to the cell you ran above.\n",
        "\n",
        "### Import packages\n",
        "8. Once the runtime has restarted, paste the following code into the new cell at the bottom of the notebook. (If no new cell exists, you can create one using the + CODE button). Press Shift + Enter to run the cell and import the necessary libraries for embedding generation, text chunking, and Firestore integration:"
      ],
      "metadata": {
        "id": "mZOgtFKYl2H1"
      },
      "id": "mZOgtFKYl2H1"
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "import logging\n",
        "import google.cloud.logging\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "\n",
        "import pickle\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "from google.cloud import firestore\n",
        "from google.cloud.firestore_v1.vector import Vector\n",
        "from google.cloud.firestore_v1.base_vector_query import DistanceMeasure"
      ],
      "metadata": {
        "id": "om49WscJlhsG"
      },
      "id": "om49WscJlhsG",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Paste the following into a new code block & run it to initialize Vertex AI."
      ],
      "metadata": {
        "id": "qh-a52tomasn"
      },
      "id": "qh-a52tomasn"
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"qwiklabs-gcp-01-205ae901edae\"\n",
        "LOCATION = \"us-central1\"\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "gRZ2AZkbmW9o"
      },
      "id": "gRZ2AZkbmW9o",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2. Load an embedding model, precomputed chunks and embeddings\n",
        "In this section, you will load the preprocessed content and corresponding text embeddings for the NYC Food Safety Manual. These were previously computed and saved in a JSON file. You'll use these chunks and embeddings as input for Retrieval-Augmented Generation (RAG) in later steps.\n",
        "\n",
        "1. Create a new code cell and run the following to load text embeddings."
      ],
      "metadata": {
        "id": "yVJBPxoDminM"
      },
      "id": "yVJBPxoDminM"
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = VertexAIEmbeddings(model_name=\"text-embedding-005\")"
      ],
      "metadata": {
        "id": "I5YrcRLkmfCK"
      },
      "id": "I5YrcRLkmfCK",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Download the preprocessed chunk and embedding data from the NYC Food Protection Training Manual."
      ],
      "metadata": {
        "id": "oqFGz65rmvAJ"
      },
      "id": "oqFGz65rmvAJ"
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud storage cp gs://qwiklabs-gcp-01-205ae901edae-bucket/embeddings.json ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhUvkxbbmpDz",
        "outputId": "3140cd3c-5d77-47f9-b45f-0493a30753e0"
      },
      "id": "dhUvkxbbmpDz",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://qwiklabs-gcp-01-205ae901edae-bucket/embeddings.json to file://./embeddings.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Run the following code in a cell to load all the chunk and embedding data from the JSON file into chunks and embeddings lists."
      ],
      "metadata": {
        "id": "-s4eIC59m1PS"
      },
      "id": "-s4eIC59m1PS"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"embeddings.json\", \"r\") as f:\n",
        "    precomputed = json.load(f)\n",
        "\n",
        "chunks = []\n",
        "embeddings = []\n",
        "\n",
        "for item in precomputed:\n",
        "    chunks.append(item[\"content\"]) # Extract the text chunk\n",
        "    embeddings.append(item[\"embedding\"]) # Extract the embedding vector"
      ],
      "metadata": {
        "id": "jpJgwqKmmyxk"
      },
      "id": "jpJgwqKmmyxk",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3. Query Firestore and Generate Answer\n",
        "In this task, you will set up a Firestore database to store the processed NYC Food Safety Manual chunks and their embeddings for efficient retrieval. You'll then build a search function to find relevant information based on a user query.\n",
        "\n",
        "1. In the Google Cloud Console, use the **Navigation menu** or the Search box to navigate to **Firestore**.\n",
        "\n",
        "2. On the Firestore Dashboard, click **CREATE A FIRESTORE DATABASE**. In the setup wizard, configure the database using the following settings, and then click the Create Database button:\n",
        "\n",
        "<table>\n",
        " <tr><td>Property</td><td>Value</td></tr>\n",
        " <tr><td>Database ID</td><td>(default)</td></tr>\n",
        " <tr><td>Edition</td><td>Standard Edition</td></tr>\n",
        " <tr><td>Configuration options</td><td>Firestore Native</td></tr>\n",
        " <tr><td>Location type</td><td>Multi-region as nam5 (United States)</td></tr>\n",
        "</table>\n",
        "\n",
        "3. Once the firestore database is created, return to the colab environment and initialize the Firestore client and store each embedding and chunk into a collection named food_safety_chunks. Paste and run the following code in a new notebook cell:"
      ],
      "metadata": {
        "id": "m_7Kq7XOnFL3"
      },
      "id": "m_7Kq7XOnFL3"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import firestore\n",
        "\n",
        "# Initialize Firestore client\n",
        "db = firestore.Client(project=PROJECT_ID)\n",
        "collection = db.collection(\"food_safety_chunks\")\n",
        "\n",
        "# Store each embedding and chunk\n",
        "for i, (embedding, chunk) in enumerate(zip(embeddings, chunks)):\n",
        "    doc = {\n",
        "        \"embedding\": embedding,\n",
        "        \"chunk\": chunk\n",
        "    }\n",
        "    collection.document(f\"chunk_{i}\").set(doc)"
      ],
      "metadata": {
        "id": "xp6S2bZim7j2"
      },
      "id": "xp6S2bZim7j2",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Embed a user query, retrieve the most relevant chunks, and generate an answer.Paste and run the following code in a new notebook cell:"
      ],
      "metadata": {
        "id": "r4ipVUC5pwKE"
      },
      "id": "r4ipVUC5pwKE"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def query_firestore(query_text):\n",
        "    model = TextEmbeddingModel.from_pretrained(\"text-embedding-005\")\n",
        "    query_embedding = model.get_embeddings([query_text])[0].values\n",
        "\n",
        "    docs = collection.stream()\n",
        "    chunks_data = [(doc.to_dict()[\"chunk\"], doc.to_dict()[\"embedding\"]) for doc in docs]\n",
        "\n",
        "    sims = [(chunk, cosine_similarity([query_embedding], [emb])[0][0]) for chunk, emb in chunks_data]\n",
        "    top_chunks = sorted(sims, key=lambda x: x[1], reverse=True)[:3]\n",
        "\n",
        "    return \"\\n\".join([chunk for chunk, _ in top_chunks])"
      ],
      "metadata": {
        "id": "cJxn_riAprNn"
      },
      "id": "cJxn_riAprNn",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Now that you can retrieve the most relevant content chunks from Firestore, use those chunks as context to generate an answer using the Gemini model. Paste and run the following code in a new notebook cell:"
      ],
      "metadata": {
        "id": "_kEy2nvgqMET"
      },
      "id": "_kEy2nvgqMET"
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What should you do if food is left out overnight?\"\n",
        "relevant_text = query_firestore(query)\n",
        "\n",
        "chat_model = GenerativeModel(\"gemini-2.0-flash\")  # Instantiate GenerativeModel directly\n",
        "chat = chat_model.start_chat()\n",
        "\n",
        "response = chat.send_message(\n",
        "    f\"Use the following to answer the question:\\n\\n{relevant_text}\\n\\nQuestion: {query}\"\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fo_cvkG_p_uT",
        "outputId": "69aa7889-fded-4b3c-c225-6f11040f9d67"
      },
      "id": "fo_cvkG_p_uT",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the information provided (food safety is crucial and proper handling ensures safety), if food is left out overnight, you should **not** consume it. Discard it to prevent potential foodborne illness.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BNxzKhVwqUxP"
      },
      "id": "BNxzKhVwqUxP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "student-01-d9201d8aec2f (Jun 20, 2025, 8:40:57 PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}