{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Compare Model Performance using the Generative AI Evaluation Service: Challenge Lab\n",
        "\n",
        "## GENAI063\n",
        "\n",
        "\n",
        "## Task 1. Initialize Gen AI in a Colab Enterprise notebook\n",
        "In this task, you will be setting up a Colab notebook and initializing Gen AI to connect the notebook and generate creative text content.\n",
        "\n",
        "1. In the Google Cloud Console, navigate to Vertex AI > Colab Enterprise.\n",
        "\n",
        "2. If prompted, enable the required APIs.\n",
        "\n",
        "3. Click on + to create a new notebook.\n",
        "\n",
        "Note: While GCP Colab Enterprise Notebooks might default to the us-central1 region, it's crucial to create your notebook in the same region where the lab environment is provisioned. You can find the lab's region on the left-hand side of the lab interface.\n",
        "\n",
        "4. Rename the notebook to cymbal-indie-film.\n",
        "\n",
        "5. Paste the following code into the top cell and run it with Shift + Return.\n",
        "\n",
        "Note: If you don’t already have an active notebook runtime, running a cell in a Colab Enterprise notebook will trigger it to create one for you and connect the notebook to it. When a runtime is allocated for the first time, you may be presented with a pop-up window to authorize the environment to act as your Qwiklabs student account.\n",
        "\n",
        "6. After the cell completes running, indicated by a checkmark to the left of the cell, the packages should be installed. To use them, we’ll restart the runtime. Click on the downward-pointing caret in the upper right of the notebook.\n",
        "\n",
        "7. Clicking on the caret should reveal a set of menus above the notebook. Select Runtime > Restart Session. When asked to confirm, select Yes. The runtime will restart, indicated by clearing the green checkmark and the cell run order integer next to the cell you ran above.\n",
        "\n",
        "8. Click + Code to add a new code and paste the following code below. Press Shift + Return to run the cell.\n",
        "\n",
        "9. In a new code block, initialize Gen AI with vertexai.init(). Use the us-central1 location and run the cell.\n",
        "\n",
        "10. Save the notebook."
      ],
      "metadata": {
        "id": "5dOLLd16plHB"
      },
      "id": "5dOLLd16plHB"
    },
    {
      "cell_type": "code",
      "id": "CssVbWZKGzllCllcZvOTNHvJ",
      "metadata": {
        "tags": [],
        "id": "CssVbWZKGzllCllcZvOTNHvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48d06c6d-554a-44c5-a53a-c86bae900d71"
      },
      "source": [
        "%pip install --upgrade --quiet google-genai nest-asyncio==1.5.9"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/222.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.8/222.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select Runtime > Restart Session."
      ],
      "metadata": {
        "id": "Hb1aitB5qM5h"
      },
      "id": "Hb1aitB5qM5h"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from inspect import cleandoc\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "from vertexai.evaluation import (\n",
        "    MetricPromptTemplateExamples,\n",
        "    EvalTask,\n",
        "    PairwiseMetric,\n",
        "    PairwiseMetricPromptTemplate,\n",
        "    PointwiseMetric,\n",
        "    PointwiseMetricPromptTemplate,\n",
        ")\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "metadata": {
        "id": "YD3i5oNEqCff"
      },
      "id": "YD3i5oNEqCff",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"qwiklabs-gcp-00-f92ddae5cd8d\"\n",
        "LOCATION = \"us-central1\"\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "63Hy_-FLqTYh"
      },
      "id": "63Hy_-FLqTYh",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zZxQKCV7qgIp"
      },
      "id": "zZxQKCV7qgIp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2. Explore example data and generate a document\n",
        "In this task, you will set up some sample data for a film production including crew rates, shooting schedules and then define questions for a large language model to answer.\n",
        "\n",
        "1. Run the following code in a new cell to instantiate some example data. The calls to cleandoc() helps remove the indents and extra lines used for making the multi-line string readable in the code."
      ],
      "metadata": {
        "id": "6Dst45gZqhPI"
      },
      "id": "6Dst45gZqhPI"
    },
    {
      "cell_type": "code",
      "source": [
        "hourly_rates = cleandoc(\"\"\"\n",
        "  Screenwriter: $40\n",
        "  Actor: $25\n",
        "  Director: $30\n",
        "  Camera Operator: $35\n",
        "  Sound Engineer: $20\n",
        "  Editor: $30\n",
        "  \"\"\")\n",
        "\n",
        "planning_notes = cleandoc(\"\"\"\n",
        " Phases of Production:\n",
        "   Writing:\n",
        "   The Screenwriter will write the script.\n",
        "   They need 72 hours to do so.\n",
        "\n",
        "\n",
        "   Pre-Production:\n",
        "   The Director needs time to analyze the script.\n",
        "   They will work on it for 36 hours.\n",
        "   The Camera Operator will join the director for 24 hours of planning.\n",
        "\n",
        "\n",
        "   Production Phase 1\n",
        "   The first three days of filming will require the director, 4 actors, the camera operator, and the sound engineer\n",
        "\n",
        "\n",
        "   Production Phase 2\n",
        "   The next three days of filming will require the director, 8 actors, the camera operator, and the sound engineer\n",
        "\n",
        "\n",
        "   Post-Production\n",
        "   The editor will take 64 hours to edit the film.\n",
        "   The director will work with the editor for 24 hours during this phase.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "bko2qehIqlqo"
      },
      "id": "bko2qehIqlqo",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Run the following code to define the content we would like the model to help us with."
      ],
      "metadata": {
        "id": "ijMgo8xxquSe"
      },
      "id": "ijMgo8xxquSe"
    },
    {
      "cell_type": "code",
      "source": [
        "tasks = [\n",
        "    \"\"\"What is the cost of each phase of production?\n",
        "    If days are mentioned, assume an 8 hour work day.\"\"\",\n",
        "\n",
        "    \"\"\"How many days will each phase require? Assume an\n",
        "    8 hour work day. If multiple people are working in parallel,\n",
        "    do not add those times together, but only use the longest time.\n",
        "    Also include a count of the total number of days of the entire\n",
        "    project.\"\"\",\n",
        "\n",
        "    \"\"\"Prepare a text schedule for all phases of the film starting\n",
        "    on Feb 3, 2025. The whole crew should be off Saturdays\n",
        "    and Sundays.\"\"\"\n",
        "]"
      ],
      "metadata": {
        "id": "ZNBk05mGqnZQ"
      },
      "id": "ZNBk05mGqnZQ",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Next, define a prompt template."
      ],
      "metadata": {
        "id": "hdJ39C-1qxsy"
      },
      "id": "hdJ39C-1qxsy"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = cleandoc(\"\"\"\n",
        "<instructions>Prepare a document to fulfill the task based on the context provided.</instructions>\n",
        "<task>{task}</task>\n",
        "<context>{context}</context>\"\"\")"
      ],
      "metadata": {
        "id": "38u5skKSqw7_"
      },
      "id": "38u5skKSqw7_",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. You will compare how the lower-cost Gemini Flash compares against Gemini Pro on these instruction tasks to determine which you should use for this project. Instantiate a model variable llm_pro to contain a generative model using gemini-2.5-pro-preview-05-06 and a model variable llm_flash to contain a generative model using gemini-2.0-flash-001.\n",
        "\n",
        "5. Add a generation configuration to each model to set the temperature to 0."
      ],
      "metadata": {
        "id": "xJI1gsZerjGG"
      },
      "id": "xJI1gsZerjGG"
    },
    {
      "cell_type": "code",
      "source": [
        "llm_pro = GenerativeModel(\n",
        "  \"gemini-2.5-pro\",\n",
        "  generation_config={\n",
        "      \"temperature\": 0,\n",
        "      \"top_p\": 0.4,\n",
        "  },\n",
        ")\n",
        "\n",
        "llm_flash = GenerativeModel(\n",
        "  \"gemini-2.0-flash-001\",\n",
        "  generation_config={\n",
        "      \"temperature\": 0,\n",
        "      \"top_p\": 0.4,\n",
        "  },\n",
        ")"
      ],
      "metadata": {
        "id": "orTXuVl1rWLj"
      },
      "id": "orTXuVl1rWLj",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Combine hourly_rates and planning_notes (with a pair of line breaks as a separator) to form a context chunk."
      ],
      "metadata": {
        "id": "bXWNpEMJrvNh"
      },
      "id": "bXWNpEMJrvNh"
    },
    {
      "cell_type": "code",
      "source": [
        "context = hourly_rates + \"\\n\\n\" + planning_notes"
      ],
      "metadata": {
        "id": "44-FuCzcrpB1"
      },
      "id": "44-FuCzcrpB1",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Using the prompt template and the context, generate a response to the second task (tasks[1]) for each model (llm_pro and llm_flash). Use the Markdown() class imported from IPython.display to wrap the response text to render Gemini's responses, which are often formatted as Markdown strings."
      ],
      "metadata": {
        "id": "6uttjW2Nr02k"
      },
      "id": "6uttjW2Nr02k"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template.format(context=context, task=str(tasks[1]))"
      ],
      "metadata": {
        "id": "B_YzmCJvrxTf"
      },
      "id": "B_YzmCJvrxTf",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check results for \"llm_pro\" model:\n",
        "Markdown(llm_pro.generate_content(prompt).text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "NBPkoIJ0r5ym",
        "outputId": "09fea197-4a92-4ec7-ea2e-a205a4345300"
      },
      "id": "NBPkoIJ0r5ym",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the context provided, here is a breakdown of the duration for each project phase and the total project duration, assuming an 8-hour workday.\n\n### **Phase Durations**\n\n*   **Writing:**\n    The Screenwriter requires 72 hours.\n    *Calculation: 72 hours / 8 hours per day = **9 days***\n\n*   **Pre-Production:**\n    The Director will work for 36 hours, and the Camera Operator will work for 24 hours in parallel. The longest duration is used.\n    *Calculation: 36 hours / 8 hours per day = **4.5 days***\n\n*   **Production Phase 1:**\n    This phase is explicitly stated to last for **3 days**.\n\n*   **Production Phase 2:**\n    This phase is explicitly stated to last for **3 days**.\n\n*   **Post-Production:**\n    The Editor will work for 64 hours, and the Director will work for 24 hours in parallel. The longest duration is used.\n    *Calculation: 64 hours / 8 hours per day = **8 days***\n\n---\n\n### **Total Project Duration**\n\nThe total number of days for the entire project is the sum of all phases.\n\n*   Writing: 9 days\n*   Pre-Production: 4.5 days\n*   Production Phase 1: 3 days\n*   Production Phase 2: 3 days\n*   Post-Production: 8 days\n\n**Total Project Days: 27.5 days**"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check results for \"llm_flash\" model:\n",
        "Markdown(llm_flash.generate_content(prompt).text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "FBkIKdLnr-I2",
        "outputId": "75b3e841-8eed-461d-d031-179ef3c0f059"
      },
      "id": "FBkIKdLnr-I2",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Project Timeline - Days Per Phase\n\nHere's a breakdown of the project timeline, calculated with 8-hour workdays and parallel tasks considered:\n\n**Phase Breakdown:**\n\n*   **Writing:**\n    *   Screenwriter: 72 hours / 8 hours/day = **9 days**\n\n*   **Pre-Production:**\n    *   Director: 36 hours / 8 hours/day = 4.5 days\n    *   Camera Operator: 24 hours / 8 hours/day = 3 days\n    *   Longest Time: 4.5 days\n    *   **Total: 4.5 days**\n\n*   **Production Phase 1:**\n    *   Director, 4 Actors, Camera Operator, Sound Engineer: 3 days\n    *   **Total: 3 days**\n\n*   **Production Phase 2:**\n    *   Director, 8 Actors, Camera Operator, Sound Engineer: 3 days\n    *   **Total: 3 days**\n\n*   **Post-Production:**\n    *   Editor: 64 hours / 8 hours/day = 8 days\n    *   Director: 24 hours / 8 hours/day = 3 days\n    *   Longest Time: 8 days\n    *   **Total: 8 days**\n\n**Total Project Days:**\n\n9 days (Writing) + 4.5 days (Pre-Production) + 3 days (Production Phase 1) + 3 days (Production Phase 2) + 8 days (Post-Production) = **27.5 days**\n"
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FwCOwx3dsDtl"
      },
      "id": "FwCOwx3dsDtl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3. Prepare the Evaluation Dataset and EvalTask\n",
        "\n",
        "In this task, you will set up the data and scoring method to evaluate the models.\n",
        "\n",
        "1. You will evaluate the models' responses against each other by using Pairwise question answering quality. Note the user input fields in curly braces in this prompt, which are required to evaluate this metrics. You will use the Gemini Pro responses as your baseline model response and your Gemini Flash responses as your responses.\n",
        "\n",
        "2. Prepare a Pandas DataFrame with the fields needed for evaluation.\n",
        "\n",
        "3. Create an EvalTask(), passing in the dataset, identifying MetricPromptTemplateExamples.Pairwise.QUESTION_ANSWERING_QUALITY as the metric you would like to be calculated, and defining an experiment name of indie-film-planning.\n",
        "\n",
        "4. Save the notebook."
      ],
      "metadata": {
        "id": "Iyb0WZTssXLB"
      },
      "id": "Iyb0WZTssXLB"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oNXBiDMr4Mi1"
      },
      "id": "oNXBiDMr4Mi1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q0_xfhVU4Ml1"
      },
      "id": "Q0_xfhVU4Ml1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BAwUKZTs4MoH"
      },
      "id": "BAwUKZTs4MoH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from inspect import cleandoc\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "from vertexai.evaluation import (\n",
        "    MetricPromptTemplateExamples,\n",
        "    EvalTask,\n",
        "    PairwiseMetric,\n",
        "    PairwiseMetricPromptTemplate,\n",
        "    PointwiseMetric,\n",
        "    PointwiseMetricPromptTemplate,\n",
        ")\n",
        "\n",
        "# Initialize Vertex AI\n",
        "# Replace 'your-project-id' and 'your-region' with your actual GCP project ID and region\n",
        "# Example: vertexai.init(project=\"my-gcp-project-12345\", location=\"us-central1\")\n",
        "vertexai.init(project=\"qwiklabs-gcp-00-f92ddae5cd8d\", location=\"us-central1\")\n",
        "\n",
        "# Define context components\n",
        "hourly_rates = cleandoc(\"\"\"\n",
        "    Screenwriter: $40\n",
        "    Actor: $25\n",
        "    Director: $30\n",
        "    Camera Operator: $35\n",
        "    Sound Engineer: $20\n",
        "    Editor: $30\n",
        "    \"\"\")\n",
        "\n",
        "planning_notes = cleandoc(\"\"\"\n",
        "    Phases of Production:\n",
        "    Writing:\n",
        "    The Screenwriter will write the script.\n",
        "    They need 72 hours to do so.\n",
        "\n",
        "\n",
        "    Pre-Production:\n",
        "    The Director needs time to analyze the script.\n",
        "    They will work on it for 36 hours.\n",
        "    The Camera Operator will join the director for 24 hours of planning.\n",
        "\n",
        "\n",
        "    Production Phase 1\n",
        "    The first three days of filming will require the director, 4 actors, the camera operator, and the sound engineer\n",
        "\n",
        "\n",
        "    Production Phase 2\n",
        "    The next three days of filming will require the director, 8 actors, the camera operator, and the sound engineer\n",
        "\n",
        "\n",
        "    Post-Production\n",
        "    The editor will take 64 hours to edit the film.\n",
        "    The director will work with the editor for 24 hours during this phase.\n",
        "\"\"\")\n",
        "\n",
        "context_text = hourly_rates + \"\\n\\n\" + planning_notes\n",
        "\n",
        "# Define the tasks\n",
        "tasks = [\n",
        "    \"\"\"What is the cost of each phase of production?\n",
        "    If days are mentioned, assume an 8 hour work day.\"\"\",\n",
        "\n",
        "    \"\"\"How many days will each phase require? Assume an\n",
        "    8 hour work day. If multiple people are working in parallel,\n",
        "    do not add those times together, but only use the longest time.\n",
        "    Also include a count of the total number of days of the entire\n",
        "    project.\"\"\",\n",
        "\n",
        "    \"\"\"Prepare a text schedule for all phases of the film starting\n",
        "    on Feb 3, 2025. The whole crew should be off Saturdays\n",
        "    and Sundays.\"\"\"\n",
        "]\n",
        "\n",
        "# Prepare the 'question' field by combining context and task\n",
        "questions_for_eval = [f\"{context_text}\\n\\n{task}\" for task in tasks]\n",
        "\n",
        "# Optional: Configuration for generation (e.g., to make responses more consistent)\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,  # Lower temperature for less creativity, more deterministic output\n",
        "    max_output_tokens=1024, # Adjust based on expected response length\n",
        ")\n",
        "\n",
        "# --- Generate baseline responses from 'gemini-pro' ---\n",
        "print(\"Generating baseline responses from 'gemini-pro' model...\")\n",
        "# baseline_model = GenerativeModel(\"gemini-pro\")\n",
        "baseline_responses = []\n",
        "\n",
        "for i, question in enumerate(questions_for_eval):\n",
        "    print(f\"  Generating baseline response for Task {i+1}...\")\n",
        "    try:\n",
        "        response = llm_pro.generate_content(\n",
        "            question,\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "        baseline_responses.append(response.text)\n",
        "        print(f\"  Baseline response for Task {i+1} generated successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error generating baseline response for Task {i+1}: {e}\")\n",
        "        baseline_responses.append(f\"ERROR: Could not generate response from gemini-pro: {e}\") # Append an error message\n",
        "\n",
        "print(\"\\nBaseline response generation complete.\")\n",
        "\n",
        "# --- Generate candidate responses from 'gemini-2.0-flash-001' ---\n",
        "print(\"\\nGenerating candidate responses from 'gemini-2.0-flash-001' model...\")\n",
        "# candidate_model = GenerativeModel(\"gemini-2.0-flash-001\")\n",
        "candidate_responses = []\n",
        "\n",
        "for i, question in enumerate(questions_for_eval):\n",
        "    print(f\"  Generating candidate response for Task {i+1}...\")\n",
        "    try:\n",
        "        response = llm_flash.generate_content(\n",
        "            question,\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "        candidate_responses.append(response.text)\n",
        "        print(f\"  Candidate response for Task {i+1} generated successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error generating candidate response for Task {i+1}: {e}\")\n",
        "        candidate_responses.append(f\"ERROR: Could not generate response from gemini-2.0-flash-001: {e}\") # Append an error message\n",
        "\n",
        "print(\"\\nCandidate response generation complete.\")\n",
        "\n",
        "\n",
        "# 2) Prepare a Pandas DataFrame with the fields needed for evaluation.\n",
        "data = {\n",
        "    'question': questions_for_eval,\n",
        "    'baseline_response': baseline_responses,\n",
        "    'candidate_response': candidate_responses,\n",
        "}\n",
        "df_eval = pd.DataFrame(data)\n",
        "\n",
        "print(\"\\nDataFrame for Evaluation:\")\n",
        "display(df_eval)\n",
        "\n",
        "# 3) Create an EvalTask(), passing in the dataset, identifying\n",
        "#    MetricPromptTemplateExamples.Pairwise.YoutubeING_QUALITY as the metric you would like to be calculated,\n",
        "#    and defining an experiment name of indie-film-planning.\n",
        "\n",
        "try:\n",
        "    eval_task = EvalTask(\n",
        "        dataset=df_eval,\n",
        "        metrics=[MetricPromptTemplateExamples.Pairwise.YoutubeING_QUALITY],\n",
        "        experiment_name=\"indie-film-planning\",\n",
        "    )\n",
        "    print(cleandoc(f\"\"\"\n",
        "        \\nEvalTask created successfully with:\n",
        "        Dataset shape: {df_eval.shape}\n",
        "        Metrics: {[metric.name for metric in eval_task.metrics]}\n",
        "        Experiment Name: {eval_task.experiment_name}\n",
        "    \"\"\"))\n",
        "\n",
        "    # To actually run the evaluation, you would uncomment the line below.\n",
        "    # eval_task.run()\n",
        "    # print(\"\\nEvaluation task initiated. You can monitor its progress in the GCP console under Generative AI > Model Evaluation.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during EvalTask creation: {e}\")\n",
        "    print(\"Please ensure your GCP project ID and region are correctly set and you have the necessary permissions.\")\n",
        "    print(\"Also, confirm that the 'dataset' columns match the requirements of the chosen metric.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRtqBGmd4Mqe",
        "outputId": "e12ad64f-a603-4c1b-c3d4-58b6fb6df50d"
      },
      "id": "KRtqBGmd4Mqe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating baseline responses from 'gemini-pro' model...\n",
            "  Generating baseline response for Task 1...\n",
            "  Error generating baseline response for Task 1: Cannot get the response text.\n",
            "Cannot get the Candidate text.\n",
            "Response candidate content has no parts (and thus no text). The candidate is likely blocked by the safety filters.\n",
            "Content:\n",
            "{\n",
            "  \"role\": \"model\"\n",
            "}\n",
            "Candidate:\n",
            "{\n",
            "  \"content\": {\n",
            "    \"role\": \"model\"\n",
            "  },\n",
            "  \"finish_reason\": \"MAX_TOKENS\"\n",
            "}\n",
            "Response:\n",
            "{\n",
            "  \"candidates\": [\n",
            "    {\n",
            "      \"content\": {\n",
            "        \"role\": \"model\"\n",
            "      },\n",
            "      \"finish_reason\": \"MAX_TOKENS\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage_metadata\": {\n",
            "    \"prompt_token_count\": 228,\n",
            "    \"total_token_count\": 1251,\n",
            "    \"prompt_tokens_details\": [\n",
            "      {\n",
            "        \"modality\": \"TEXT\",\n",
            "        \"token_count\": 228\n",
            "      }\n",
            "    ],\n",
            "    \"thoughts_token_count\": 1023\n",
            "  },\n",
            "  \"model_version\": \"gemini-2.5-pro\",\n",
            "  \"create_time\": \"2025-06-28T00:35:23.574150Z\",\n",
            "  \"response_id\": \"yzhfaMaFI-G9qsMP1oSv2QU\"\n",
            "}\n",
            "  Generating baseline response for Task 2...\n",
            "  Error generating baseline response for Task 2: Cannot get the response text.\n",
            "Cannot get the Candidate text.\n",
            "Response candidate content has no parts (and thus no text). The candidate is likely blocked by the safety filters.\n",
            "Content:\n",
            "{\n",
            "  \"role\": \"model\"\n",
            "}\n",
            "Candidate:\n",
            "{\n",
            "  \"content\": {\n",
            "    \"role\": \"model\"\n",
            "  },\n",
            "  \"finish_reason\": \"MAX_TOKENS\"\n",
            "}\n",
            "Response:\n",
            "{\n",
            "  \"candidates\": [\n",
            "    {\n",
            "      \"content\": {\n",
            "        \"role\": \"model\"\n",
            "      },\n",
            "      \"finish_reason\": \"MAX_TOKENS\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage_metadata\": {\n",
            "    \"prompt_token_count\": 263,\n",
            "    \"total_token_count\": 1286,\n",
            "    \"prompt_tokens_details\": [\n",
            "      {\n",
            "        \"modality\": \"TEXT\",\n",
            "        \"token_count\": 263\n",
            "      }\n",
            "    ],\n",
            "    \"thoughts_token_count\": 1023\n",
            "  },\n",
            "  \"model_version\": \"gemini-2.5-pro\",\n",
            "  \"create_time\": \"2025-06-28T00:35:32.197442Z\",\n",
            "  \"response_id\": \"1DhfaMKGDI6XhMIP2OqFyAc\"\n",
            "}\n",
            "  Generating baseline response for Task 3...\n",
            "  Error generating baseline response for Task 3: Cannot get the response text.\n",
            "Cannot get the Candidate text.\n",
            "Response candidate content has no parts (and thus no text). The candidate is likely blocked by the safety filters.\n",
            "Content:\n",
            "{\n",
            "  \"role\": \"model\"\n",
            "}\n",
            "Candidate:\n",
            "{\n",
            "  \"content\": {\n",
            "    \"role\": \"model\"\n",
            "  },\n",
            "  \"finish_reason\": \"MAX_TOKENS\"\n",
            "}\n",
            "Response:\n",
            "{\n",
            "  \"candidates\": [\n",
            "    {\n",
            "      \"content\": {\n",
            "        \"role\": \"model\"\n",
            "      },\n",
            "      \"finish_reason\": \"MAX_TOKENS\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage_metadata\": {\n",
            "    \"prompt_token_count\": 239,\n",
            "    \"total_token_count\": 1262,\n",
            "    \"prompt_tokens_details\": [\n",
            "      {\n",
            "        \"modality\": \"TEXT\",\n",
            "        \"token_count\": 239\n",
            "      }\n",
            "    ],\n",
            "    \"thoughts_token_count\": 1023\n",
            "  },\n",
            "  \"model_version\": \"gemini-2.5-pro\",\n",
            "  \"create_time\": \"2025-06-28T00:35:42.059408Z\",\n",
            "  \"response_id\": \"3jhfaJDQA_TehMIPsay6iQU\"\n",
            "}\n",
            "\n",
            "Baseline response generation complete.\n",
            "\n",
            "Generating candidate responses from 'gemini-2.0-flash-001' model...\n",
            "  Generating candidate response for Task 1...\n",
            "  Candidate response for Task 1 generated successfully.\n",
            "  Generating candidate response for Task 2...\n",
            "  Candidate response for Task 2 generated successfully.\n",
            "  Generating candidate response for Task 3...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KSXzY-2_4Msw"
      },
      "id": "KSXzY-2_4Msw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iwicMgrb4MwL"
      },
      "id": "iwicMgrb4MwL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # === 2. Construct evaluation examples ===\n",
        "# examples = []\n",
        "# for task in tasks:\n",
        "#     input_prompt = prompt_template.format(task=task, context=context)\n",
        "#     examples.append({\n",
        "#         \"input\": input_prompt\n",
        "#     })\n",
        "\n",
        "# # === 3. Create EvalTask ===\n",
        "# eval_task = EvalTask(\n",
        "#     # name=\"film_schedule_pairwise_eval\",\n",
        "#     # metric=PairwiseQuestionAnsweringQuality(),\n",
        "#     metrics=[\n",
        "#         MetricPromptTemplateExamples.Pairwise.QUESTION_ANSWERING_QUALITY\n",
        "#     ],\n",
        "#     dataset=examples,\n",
        "#     experiment=\"indie-film-planning\",\n",
        "#     # input_data=examples,\n",
        "#     model=\"gemini-2.0-flash-001\",\n",
        "#     baseline_model=\"gemini-2.5-pro\"\n",
        "# )"
      ],
      "metadata": {
        "id": "dqme6lLOsfAX"
      },
      "id": "dqme6lLOsfAX",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric_prompt_template = \"\"\"\n",
        "# Instruction\n",
        "You are an expert evaluator. Your task is to evaluate the quality of the responses generated by two AI models. We will provide you with the user input and a pair of AI-generated responses (Response A and Response B). You should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.\n",
        "\n",
        "You will first judge responses individually, following the Rating Rubric and Evaluation Steps. Then you will give step-by-step explanations for your judgment, compare results to declare the winner based on the Rating Rubric and Evaluation Steps.\n",
        "\n",
        "# Evaluation\n",
        "## Metric Definition\n",
        "You will be assessing question answering quality, which measures the overall quality of the answer to the question in the user prompt. Pay special attention to length constraints, such as in X words or in Y sentences. The instruction for performing a question-answering task is provided in the user prompt. The response should not contain information that is not present in the context (if it is provided).\n",
        "\n",
        "## Criteria\n",
        "Instruction following: The response demonstrates a clear understanding of the question answering task instructions, satisfying all of the instruction's requirements.\n",
        "Groundedness: The response contains information included only in the context if the context is present in the user prompt. The response does not reference any outside information.\n",
        "Completeness: The response completely answers the question with sufficient detail.\n",
        "Fluent: The response is well-organized and easy to read.\n",
        "\n",
        "## Rating Rubric\n",
        "\"A\": Response A answers the given question as per the criteria better than response B.\n",
        "\"SAME\": Response A and B answers the given question equally well as per the criteria.\n",
        "\"B\": Response B answers the given question as per the criteria better than response A.\n",
        "\n",
        "## Evaluation Steps\n",
        "STEP 1: Analyze Response A based on the question answering quality criteria: Determine how well Response A fulfills the user requirements, is grounded in the context, is complete and fluent, and provides assessment according to the criterion.\n",
        "STEP 2: Analyze Response B based on the question answering quality criteria: Determine how well Response B fulfills the user requirements, is grounded in the context, is complete and fluent, and provides assessment according to the criterion.\n",
        "STEP 3: Compare the overall performance of Response A and Response B based on your analyses and assessment.\n",
        "STEP 4: Output your preference of \"A\", \"SAME\" or \"B\" to the pairwise_choice field according to the Rating Rubric.\n",
        "STEP 5: Output your assessment reasoning in the explanation field.\n",
        "\n",
        "# User Inputs and AI-generated Responses\n",
        "## User Inputs\n",
        "### Prompt\n",
        "{prompt}\n",
        "\n",
        "# AI-generated Response\n",
        "\n",
        "### Response A\n",
        "{baseline_model_response}\n",
        "\n",
        "### Response B\n",
        "{response}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nlozxOxkx1ih"
      },
      "id": "nlozxOxkx1ih",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Summarize the following article\"\n",
        "\n",
        "context = [\n",
        "    \"To make a classic spaghetti carbonara, start by bringing a large pot of salted water to a boil. While the water is heating up, cook pancetta or guanciale in a skillet with olive oil over medium heat until it's crispy and golden brown. Once the pancetta is done, remove it from the skillet and set it aside. In the same skillet, whisk together eggs, grated Parmesan cheese, and black pepper to make the sauce. When the pasta is cooked al dente, drain it and immediately toss it in the skillet with the egg mixture, adding a splash of the pasta cooking water to create a creamy sauce.\",\n",
        "    \"Preparing a perfect risotto requires patience and attention to detail. Begin by heating butter in a large, heavy-bottomed pot over medium heat. Add finely chopped onions and minced garlic to the pot, and cook until they're soft and translucent, about 5 minutes. Next, add Arborio rice to the pot and cook, stirring constantly, until the grains are coated with the butter and begin to toast slightly. Pour in a splash of white wine and cook until it's absorbed. From there, gradually add hot chicken or vegetable broth to the rice, stirring frequently, until the risotto is creamy and the rice is tender with a slight bite.\",\n",
        "    \"For a flavorful grilled steak, start by choosing a well-marbled cut of beef like ribeye or New York strip. Season the steak generously with kosher salt and freshly ground black pepper on both sides, pressing the seasoning into the meat. Preheat a grill to high heat and brush the grates with oil to prevent sticking. Place the seasoned steak on the grill and cook for about 4-5 minutes on each side for medium-rare, or adjust the cooking time to your desired level of doneness. Let the steak rest for a few minutes before slicing against the grain and serving.\",\n",
        "    \"Creating a creamy homemade tomato soup is a comforting and simple process. Begin by heating olive oil in a large pot over medium heat. Add diced onions and minced garlic to the pot and cook until they're soft and fragrant. Next, add chopped fresh tomatoes, chicken or vegetable broth, and a sprig of fresh basil to the pot. Simmer the soup for about 20-30 minutes, or until the tomatoes are tender and falling apart. Remove the basil sprig and use an immersion blender to puree the soup until smooth. Season with salt and pepper to taste before serving.\",\n",
        "    \"To bake a decadent chocolate cake from scratch, start by preheating your oven to 350°F (175°C) and greasing and flouring two 9-inch round cake pans. In a large mixing bowl, cream together softened butter and granulated sugar until light and fluffy. Beat in eggs one at a time, making sure each egg is fully incorporated before adding the next. In a separate bowl, sift together all-purpose flour, cocoa powder, baking powder, baking soda, and salt. Divide the batter evenly between the prepared cake pans and bake for 25-30 minutes, or until a toothpick inserted into the center comes out clean.\",\n",
        "]\n",
        "\n",
        "reference = [\n",
        "    \"The process of making spaghetti carbonara involves boiling pasta, crisping pancetta or guanciale, whisking together eggs and Parmesan cheese, and tossing everything together to create a creamy sauce.\",\n",
        "    \"Preparing risotto entails sautéing onions and garlic, toasting Arborio rice, adding wine and broth gradually, and stirring until creamy and tender.\",\n",
        "    \"Grilling a flavorful steak involves seasoning generously, preheating the grill, cooking to desired doneness, and letting it rest before slicing.\",\n",
        "    \"Creating homemade tomato soup includes sautéing onions and garlic, simmering with tomatoes and broth, pureeing until smooth, and seasoning to taste.\",\n",
        "    \"Baking a decadent chocolate cake requires creaming butter and sugar, beating in eggs and alternating dry ingredients with buttermilk before baking until done.\",\n",
        "]\n",
        "\n",
        "eval_dataset = pd.DataFrame(\n",
        "    {\n",
        "        \"context\": context,\n",
        "        \"reference\": reference,\n",
        "        \"instruction\": [instruction] * len(context),\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "eIg-mLWV3VvS"
      },
      "id": "eIg-mLWV3VvS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = \"indie-film-planning\"\n",
        "\n",
        "metrics = [\"MetricPromptTemplateExamples.Pairwise.QUESTION_ANSWERING_QUALITY\"]\n",
        "\n",
        "summarization_eval_task = EvalTask(\n",
        "    dataset=eval_dataset,\n",
        "    metrics=metrics,\n",
        "    experiment=experiment_name,\n",
        ")"
      ],
      "metadata": {
        "id": "2XlW0WoXz7VK"
      },
      "id": "2XlW0WoXz7VK",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "student-01-0316e2aeb8cb (Jun 27, 2025, 4:07:56 PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}